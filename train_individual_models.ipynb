{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train_individual_models.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "AAphn-KTcYRT",
        "oNMUd8p5c1ia",
        "raLqusqc-HUO",
        "ozMhFVbJ-axW",
        "PI5gU66udZ9z",
        "FxK-zulhAabC",
        "1_b5xxMz-Siy",
        "VUKuV589-0Dz",
        "5QB1JiORc-_J",
        "6agcWVSsdYA6",
        "FS1vY39jddg-",
        "ytz64QU1EzcE",
        "vewK3OwGoESG",
        "F1ieNfPNoGGp",
        "15sywLwhoVVb",
        "S3JlxE0xobTQ",
        "UGl_QbmMonwA",
        "KTZxQ_9cE0Yf",
        "6fZT8kFvory0",
        "kS27O-NIX7kZ"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3a5bb1c2e10847cbb04e6a703bf17c28": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_0ba4c5f24a0c4b97ae96229228a0daea",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_0c10db0e76a34c9c810550cc2ce80a4d",
              "IPY_MODEL_210d93ccd52c46a28e258fb04be39c19"
            ]
          }
        },
        "0ba4c5f24a0c4b97ae96229228a0daea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0c10db0e76a34c9c810550cc2ce80a4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_8db5187646864526bdaa281297a98236",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 574673361,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 574673361,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_daad4ffa3edc4f168c47f07c01af2138"
          }
        },
        "210d93ccd52c46a28e258fb04be39c19": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_3c3d467680a24822bbea50937d98c903",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 548M/548M [00:10&lt;00:00, 52.6MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_13a626f1f9f747b5b727982b921d223c"
          }
        },
        "8db5187646864526bdaa281297a98236": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "daad4ffa3edc4f168c47f07c01af2138": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3c3d467680a24822bbea50937d98c903": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "13a626f1f9f747b5b727982b921d223c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AAphn-KTcYRT",
        "colab_type": "text"
      },
      "source": [
        "# Authenticate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jt47JXK7qSoP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNMUd8p5c1ia",
        "colab_type": "text"
      },
      "source": [
        "# Load data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "raLqusqc-HUO",
        "colab_type": "text"
      },
      "source": [
        "## Install Cloud Storage FUSE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRoR9kM0yi_3",
        "colab_type": "code",
        "outputId": "44c3e4aa-796e-40d6-d78b-9a97389e7e04",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        }
      },
      "source": [
        "!echo \"deb http://packages.cloud.google.com/apt gcsfuse-bionic main\" > /etc/apt/sources.list.d/gcsfuse.list\n",
        "!curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -\n",
        "!apt -qq update\n",
        "!apt -qq install gcsfuse"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100   653  100   653    0     0   7867      0 --:--:-- --:--:-- --:--:--  7963\n",
            "OK\n",
            "52 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "The following NEW packages will be installed:\n",
            "  gcsfuse\n",
            "0 upgraded, 1 newly installed, 0 to remove and 52 not upgraded.\n",
            "Need to get 4,274 kB of archives.\n",
            "After this operation, 12.8 MB of additional disk space will be used.\n",
            "Selecting previously unselected package gcsfuse.\n",
            "(Reading database ... 144568 files and directories currently installed.)\n",
            "Preparing to unpack .../gcsfuse_0.28.1_amd64.deb ...\n",
            "Unpacking gcsfuse (0.28.1) ...\n",
            "Setting up gcsfuse (0.28.1) ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozMhFVbJ-axW",
        "colab_type": "text"
      },
      "source": [
        "## Create connection between Cloud Storage and Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCIJtvHBzIAx",
        "colab_type": "code",
        "outputId": "6b869701-7caf-47cc-e45d-3a424905216f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "!mkdir celeb-df\n",
        "!gcsfuse celeb-df celeb-df\n",
        "!ls celeb-df\n",
        "!ls celeb-df | wc"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using mount point: /content/celeb-df\n",
            "Opening GCS connection...\n",
            "Opening bucket...\n",
            "Mounting file system...\n",
            "File system has been successfully mounted.\n",
            "test  train  validation\n",
            "      3       3      22\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PI5gU66udZ9z",
        "colab_type": "text"
      },
      "source": [
        "# Train Capsule model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hod36IhHA4b9",
        "colab_type": "text"
      },
      "source": [
        "**Public repository:** \n",
        "https://github.com/nii-yamagishilab/Capsule-Forensics-v2\n",
        "\n",
        "**Reference:**\n",
        "H. H. Nguyen, J. Yamagishi, and I. Echizen, “Use of a Capsule Network to Detect Fake Images and Videos,” arXiv preprint arXiv:1910.12467. 2019 Oct 29."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FxK-zulhAabC",
        "colab_type": "text"
      },
      "source": [
        "## 1. Import dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-XzVj-mmYxo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "import torch.backends.cudnn as cudnn\n",
        "import numpy as np\n",
        "from torch.autograd import Variable\n",
        "from torch.optim import Adam\n",
        "import torchvision.models as models\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "from tqdm import tqdm\n",
        "from sklearn import metrics"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_b5xxMz-Siy",
        "colab_type": "text"
      },
      "source": [
        "## 2. Capsule Network architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8I8i66agmSV9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NO_CAPS = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2EVEaLvmXIU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class StatsNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(StatsNet, self).__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.data.shape[0], x.data.shape[1], x.data.shape[2]*x.data.shape[3])\n",
        "\n",
        "        mean = torch.mean(x, 2)\n",
        "        std = torch.std(x, 2)\n",
        "\n",
        "        return torch.stack((mean, std), dim=1)\n",
        "\n",
        "class View(nn.Module):\n",
        "    def __init__(self, *shape):\n",
        "        super(View, self).__init__()\n",
        "        self.shape = shape\n",
        "\n",
        "    def forward(self, input):\n",
        "        return input.view(self.shape)\n",
        "\n",
        "class VggExtractor(nn.Module):\n",
        "    def __init__(self, train=False):\n",
        "        super(VggExtractor, self).__init__()\n",
        "\n",
        "        self.vgg_1 = self.Vgg(models.vgg19(pretrained=True), 0, 18)\n",
        "        if train:\n",
        "            self.vgg_1.train(mode=True)\n",
        "            self.freeze_gradient()\n",
        "        else:\n",
        "            self.vgg_1.eval()\n",
        "\n",
        "    def Vgg(self, vgg, begin, end):\n",
        "        features = nn.Sequential(*list(vgg.features.children())[begin:(end+1)])\n",
        "        return features\n",
        "\n",
        "    def freeze_gradient(self, begin=0, end=9):\n",
        "        for i in range(begin, end+1):\n",
        "            self.vgg_1[i].requires_grad = False\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.vgg_1(input)\n",
        "\n",
        "class FeatureExtractor(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FeatureExtractor, self).__init__()\n",
        "\n",
        "        self.capsules = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Conv2d(256, 64, kernel_size=3, stride=1, padding=1),\n",
        "                nn.BatchNorm2d(64),\n",
        "                nn.ReLU(),\n",
        "                nn.Conv2d(64, 16, kernel_size=3, stride=1, padding=1),\n",
        "                nn.BatchNorm2d(16),\n",
        "                nn.ReLU(),\n",
        "                StatsNet(),\n",
        "\n",
        "                nn.Conv1d(2, 8, kernel_size=5, stride=2, padding=2),\n",
        "                nn.BatchNorm1d(8),\n",
        "                nn.Conv1d(8, 1, kernel_size=3, stride=1, padding=1),\n",
        "                nn.BatchNorm1d(1),\n",
        "                View(-1, 8),\n",
        "                )\n",
        "                for _ in range(NO_CAPS)]\n",
        "        )\n",
        "\n",
        "    def squash(self, tensor, dim):\n",
        "        squared_norm = (tensor ** 2).sum(dim=dim, keepdim=True)\n",
        "        scale = squared_norm / (1 + squared_norm)\n",
        "        return scale * tensor / (torch.sqrt(squared_norm))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # outputs = [capsule(x.detach()) for capsule in self.capsules]\n",
        "        # outputs = [capsule(x.clone()) for capsule in self.capsules]\n",
        "        outputs = [capsule(x) for capsule in self.capsules]\n",
        "        output = torch.stack(outputs, dim=-1)\n",
        "\n",
        "        return self.squash(output, dim=-1)\n",
        "\n",
        "class RoutingLayer(nn.Module):\n",
        "    def __init__(self, gpu_id, num_input_capsules, num_output_capsules, data_in, data_out, num_iterations):\n",
        "        super(RoutingLayer, self).__init__()\n",
        "\n",
        "        self.gpu_id = gpu_id\n",
        "        self.num_iterations = num_iterations\n",
        "        self.route_weights = nn.Parameter(torch.randn(num_output_capsules, num_input_capsules, data_out, data_in))\n",
        "\n",
        "\n",
        "    def squash(self, tensor, dim):\n",
        "        squared_norm = (tensor ** 2).sum(dim=dim, keepdim=True)\n",
        "        scale = squared_norm / (1 + squared_norm)\n",
        "        return scale * tensor / (torch.sqrt(squared_norm))\n",
        "\n",
        "    def forward(self, x, random, dropout):\n",
        "        # x[b, data, in_caps]\n",
        "\n",
        "        x = x.transpose(2, 1)\n",
        "        # x[b, in_caps, data]\n",
        "\n",
        "        if random:\n",
        "            noise = Variable(0.01*torch.randn(*self.route_weights.size()))\n",
        "            if self.gpu_id >= 0:\n",
        "                noise = noise.cuda(self.gpu_id)\n",
        "            route_weights = self.route_weights + noise\n",
        "        else:\n",
        "            route_weights = self.route_weights\n",
        "\n",
        "        priors = route_weights[:, None, :, :, :] @ x[None, :, :, :, None]\n",
        "\n",
        "        # route_weights [out_caps , 1 , in_caps , data_out , data_in]\n",
        "        # x             [   1     , b , in_caps , data_in ,    1    ]\n",
        "        # priors        [out_caps , b , in_caps , data_out,    1    ]\n",
        "\n",
        "        priors = priors.transpose(1, 0)\n",
        "        # priors[b, out_caps, in_caps, data_out, 1]\n",
        "\n",
        "        if dropout > 0.0:\n",
        "            drop = Variable(torch.FloatTensor(*priors.size()).bernoulli(1.0- dropout))\n",
        "            if self.gpu_id >= 0:\n",
        "                drop = drop.cuda(self.gpu_id)\n",
        "            priors = priors * drop\n",
        "            \n",
        "\n",
        "        logits = Variable(torch.zeros(*priors.size()))\n",
        "        # logits[b, out_caps, in_caps, data_out, 1]\n",
        "\n",
        "        if self.gpu_id >= 0:\n",
        "            logits = logits.cuda(self.gpu_id)\n",
        "\n",
        "        num_iterations = self.num_iterations\n",
        "\n",
        "        for i in range(num_iterations):\n",
        "            probs = F.softmax(logits, dim=2)\n",
        "            outputs = self.squash((probs * priors).sum(dim=2, keepdim=True), dim=3)\n",
        "\n",
        "            if i != self.num_iterations - 1:\n",
        "                delta_logits = priors * outputs\n",
        "                logits = logits + delta_logits\n",
        "\n",
        "        # outputs[b, out_caps, 1, data_out, 1]\n",
        "        outputs = outputs.squeeze()\n",
        "\n",
        "        if len(outputs.shape) == 3:\n",
        "            outputs = outputs.transpose(2, 1).contiguous() \n",
        "        else:\n",
        "            outputs = outputs.unsqueeze_(dim=0).transpose(2, 1).contiguous()\n",
        "        # outputs[b, data_out, out_caps]\n",
        "\n",
        "        return outputs\n",
        "\n",
        "class CapsuleNet(nn.Module):\n",
        "    def __init__(self, num_class, gpu_id):\n",
        "        super(CapsuleNet, self).__init__()\n",
        "\n",
        "        self.num_class = num_class\n",
        "        self.fea_ext = FeatureExtractor()\n",
        "        self.fea_ext.apply(self.weights_init)\n",
        "\n",
        "        self.routing_stats = RoutingLayer(gpu_id=gpu_id, num_input_capsules=NO_CAPS, num_output_capsules=num_class, data_in=8, data_out=4, num_iterations=2)\n",
        "\n",
        "    def weights_init(self, m):\n",
        "        classname = m.__class__.__name__\n",
        "        if classname.find('Conv') != -1:\n",
        "            m.weight.data.normal_(0.0, 0.02)\n",
        "        elif classname.find('BatchNorm') != -1:\n",
        "            m.weight.data.normal_(1.0, 0.02)\n",
        "            m.bias.data.fill_(0)\n",
        "\n",
        "    def forward(self, x, random=False, dropout=0.0):\n",
        "\n",
        "        z = self.fea_ext(x)\n",
        "        z = self.routing_stats(z, random, dropout=dropout)\n",
        "        # z[b, data, out_caps]\n",
        "\n",
        "        # classes = F.softmax(z, dim=-1)\n",
        "\n",
        "        # class_ = classes.detach()\n",
        "        # class_ = class_.mean(dim=1)\n",
        "\n",
        "        # return classes, class_\n",
        "\n",
        "        classes = F.softmax(z, dim=-1)\n",
        "        class_ = classes.detach()\n",
        "        class_ = class_.mean(dim=1)\n",
        "\n",
        "        return z, class_\n",
        "\n",
        "class CapsuleLoss(nn.Module):\n",
        "    def __init__(self, gpu_id):\n",
        "        super(CapsuleLoss, self).__init__()\n",
        "        self.cross_entropy_loss = nn.CrossEntropyLoss()\n",
        "\n",
        "        if gpu_id >= 0:\n",
        "            self.cross_entropy_loss.cuda(gpu_id)\n",
        "\n",
        "    def forward(self, classes, labels):\n",
        "        loss_t = self.cross_entropy_loss(classes[:,0,:], labels)\n",
        "\n",
        "        for i in range(classes.size(1) - 1):\n",
        "            loss_t = loss_t + self.cross_entropy_loss(classes[:,i+1,:], labels)\n",
        "\n",
        "        return loss_t"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUKuV589-0Dz",
        "colab_type": "text"
      },
      "source": [
        "## 3. Default settings\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kAyIs5gJoTLH",
        "colab_type": "code",
        "outputId": "ab3ad061-f3fe-429a-b55d-860381ef2f92",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "dataset = os.getcwd() + '/celeb-df'\n",
        "train_set = '/train'\n",
        "val_set = '/validation'\n",
        "test_set = '/test'\n",
        "workers = 0\n",
        "batch_size = 32\n",
        "image_size = 300\n",
        "learning_rate = 0.0005\n",
        "beta1_for_adam = 0.9\n",
        "gpu_id = 0\n",
        "manual_seed = random.randint(1, 1000)\n",
        "print(\"Random Seed: \", manual_seed)\n",
        "random.seed(manual_seed)\n",
        "torch.manual_seed(manual_seed)\n",
        "torch.cuda.manual_seed_all(manual_seed)\n",
        "cudnn.benchmark = True\n",
        "disable_random_routing_matrix = False\n",
        "random_setting = not disable_random_routing_matrix\n",
        "text_writer_train = open(os.path.join(os.getcwd(), 'train.csv'), 'a')\n",
        "text_writer_test = open(os.path.join(os.getcwd(), 'test.txt'), 'w')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Random Seed:  287\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QB1JiORc-_J",
        "colab_type": "text"
      },
      "source": [
        "## 4. Creating Capsule"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Omwg2p0bnqmC",
        "colab_type": "code",
        "outputId": "4b39f943-998a-4092-962a-076a711b0227",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136,
          "referenced_widgets": [
            "3a5bb1c2e10847cbb04e6a703bf17c28",
            "0ba4c5f24a0c4b97ae96229228a0daea",
            "0c10db0e76a34c9c810550cc2ce80a4d",
            "210d93ccd52c46a28e258fb04be39c19",
            "8db5187646864526bdaa281297a98236",
            "daad4ffa3edc4f168c47f07c01af2138",
            "3c3d467680a24822bbea50937d98c903",
            "13a626f1f9f747b5b727982b921d223c"
          ]
        }
      },
      "source": [
        "vgg_ext = VggExtractor()\n",
        "capnet = CapsuleNet(2, gpu_id)\n",
        "capsule_loss = CapsuleLoss(gpu_id)\n",
        "optimizer = Adam(capnet.parameters(), lr=learning_rate, betas=(beta1_for_adam, 0.999))\n",
        "\n",
        "capnet.cuda(gpu_id)\n",
        "vgg_ext.cuda(gpu_id)\n",
        "capsule_loss.cuda(gpu_id)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/checkpoints/vgg19-dcbb9e9d.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3a5bb1c2e10847cbb04e6a703bf17c28",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, max=574673361), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CapsuleLoss(\n",
              "  (cross_entropy_loss): CrossEntropyLoss()\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6agcWVSsdYA6",
        "colab_type": "text"
      },
      "source": [
        "## 5. Creating datasets for training, validation, and test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8PkE7_INQx7",
        "colab_type": "code",
        "outputId": "730a6468-fe4d-446e-b4fb-69a8fcce1289",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        }
      },
      "source": [
        "transform_fwd = transforms.Compose([\n",
        "        transforms.Resize((image_size, image_size)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
        "        ])\n",
        "\n",
        "dataset_train = dset.ImageFolder(root=dataset + train_set, transform=transform_fwd)\n",
        "assert dataset_train\n",
        "dataloader_train = torch.utils.data.DataLoader(dataset_train, batch_size=batch_size, shuffle=True, num_workers=int(workers))\n",
        "\n",
        "dataset_val = dset.ImageFolder(root=dataset + val_set, transform=transform_fwd)\n",
        "assert dataset_val\n",
        "dataloader_val = torch.utils.data.DataLoader(dataset_val, batch_size=batch_size, shuffle=False, num_workers=int(workers))\n",
        "\n",
        "dataset_test = dset.ImageFolder(root=dataset + test_set, transform=transform_fwd)\n",
        "assert dataset_test\n",
        "dataloader_test = torch.utils.data.DataLoader(dataset_test, batch_size=batch_size, shuffle=False, num_workers=int(workers))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-d2598c804728>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m         ])\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdataset_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform_fwd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mdataset_train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mdataloader_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file)\u001b[0m\n\u001b[1;32m    207\u001b[0m                                           \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m                                           \u001b[0mtarget_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m                                           is_valid_file=is_valid_file)\n\u001b[0m\u001b[1;32m    210\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m             raise (RuntimeError(\"Found 0 files in subfolders of: \" + self.root + \"\\n\"\n\u001b[0;32m---> 97\u001b[0;31m                                 \"Supported extensions are: \" + \",\".join(extensions)))\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Found 0 files in subfolders of: /content/celeb-df/train\nSupported extensions are: .jpg,.jpeg,.png,.ppm,.bmp,.pgm,.tif,.tiff,.webp"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FS1vY39jddg-",
        "colab_type": "text"
      },
      "source": [
        "## 6. Start training phase\n",
        "- Save the fitted model at the end"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGWdzEJ4QKLJ",
        "colab_type": "code",
        "outputId": "209ce60a-fa08-4229-9e9d-d7279af08772",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for epoch in range(1, 26):\n",
        "        count = 0\n",
        "        loss_train = 0\n",
        "        loss_test = 0\n",
        "\n",
        "        tol_label = np.array([], dtype=np.float)\n",
        "        tol_pred = np.array([], dtype=np.float)\n",
        "\n",
        "        for img_data, labels_data in tqdm(dataloader_train):\n",
        "\n",
        "            labels_data[labels_data > 1] = 1\n",
        "            img_label = labels_data.numpy().astype(np.float)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            if gpu_id >= 0:\n",
        "              img_data = img_data.cuda(gpu_id)\n",
        "              labels_data = labels_data.cuda(gpu_id)\n",
        "\n",
        "            input_v = Variable(img_data)\n",
        "            x = vgg_ext(input_v)\n",
        "            classes, class_ = capnet(x, random=optrandom, dropout=0.05)\n",
        "\n",
        "            loss_dis = capsule_loss(classes, Variable(labels_data, requires_grad=False))\n",
        "            loss_dis_data = loss_dis.item()\n",
        "\n",
        "            loss_dis.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            output_dis = class_.data.cpu().numpy()\n",
        "            output_pred = np.zeros((output_dis.shape[0]), dtype=np.float)\n",
        "\n",
        "            for i in range(output_dis.shape[0]):\n",
        "                if output_dis[i,1] >= output_dis[i,0]:\n",
        "                    output_pred[i] = 1.0\n",
        "                else:\n",
        "                    output_pred[i] = 0.0\n",
        "\n",
        "            tol_label = np.concatenate((tol_label, img_label))\n",
        "            tol_pred = np.concatenate((tol_pred, output_pred))\n",
        "\n",
        "            loss_train += loss_dis_data\n",
        "            count += 1\n",
        "\n",
        "\n",
        "        acc_train = metrics.accuracy_score(tol_label, tol_pred)\n",
        "        loss_train /= count\n",
        "\n",
        "        ########################################################################\n",
        "\n",
        "        # do checkpointing & validation\n",
        "        torch.save(capnet.state_dict(), os.path.join(os.getcwd(), 'capsule_%d.pt' % epoch))\n",
        "        torch.save(optimizer.state_dict(), os.path.join(os.getcwd(), 'optim_%d.pt' % epoch))\n",
        "\n",
        "        capnet.eval()\n",
        "\n",
        "        tol_label = np.array([], dtype=np.float)\n",
        "        tol_pred = np.array([], dtype=np.float)\n",
        "\n",
        "        count = 0\n",
        "\n",
        "        for img_data, labels_data in dataloader_val:\n",
        "\n",
        "            labels_data[labels_data > 1] = 1\n",
        "            img_label = labels_data.numpy().astype(np.float)\n",
        "\n",
        "            if gpu_id >= 0:\n",
        "              img_data = img_data.cuda(gpu_id)\n",
        "              labels_data = labels_data.cuda(gpu_id)\n",
        "\n",
        "            input_v = Variable(img_data)\n",
        "\n",
        "            x = vgg_ext(input_v)\n",
        "            classes, class_ = capnet(x, random=False)\n",
        "\n",
        "            loss_dis = capsule_loss(classes, Variable(labels_data, requires_grad=False))\n",
        "            loss_dis_data = loss_dis.item()\n",
        "            output_dis = class_.data.cpu().numpy()\n",
        "\n",
        "            output_pred = np.zeros((output_dis.shape[0]), dtype=np.float)\n",
        "\n",
        "            for i in range(output_dis.shape[0]):\n",
        "                if output_dis[i,1] >= output_dis[i,0]:\n",
        "                    output_pred[i] = 1.0\n",
        "                else:\n",
        "                    output_pred[i] = 0.0\n",
        "\n",
        "            tol_label = np.concatenate((tol_label, img_label))\n",
        "            tol_pred = np.concatenate((tol_pred, output_pred))\n",
        "\n",
        "            loss_test += loss_dis_data\n",
        "            count += 1\n",
        "\n",
        "        acc_test = metrics.accuracy_score(tol_label, tol_pred)\n",
        "        loss_test /= count\n",
        "\n",
        "        print('[Epoch %d] Train loss: %.4f   acc: %.2f | Test loss: %.4f  acc: %.2f'\n",
        "        % (epoch, loss_train, acc_train*100, loss_test, acc_test*100))\n",
        "\n",
        "        text_writer.write('%d,%.4f,%.2f,%.4f,%.2f\\n'\n",
        "        % (epoch, loss_train, acc_train*100, loss_test, acc_test*100))\n",
        "\n",
        "        text_writer.flush()\n",
        "        capnet.train(mode=True)\n",
        "\n",
        "text_writer.close()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  1.34it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch 1] Train loss: 2.7654   acc: 100.00 | Test loss: 2.7647  acc: 100.00\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  1.47it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch 2] Train loss: 1.6929   acc: 100.00 | Test loss: 2.7656  acc: 100.00\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  1.46it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch 3] Train loss: 1.6828   acc: 100.00 | Test loss: 2.7663  acc: 100.00\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  1.48it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch 4] Train loss: 1.6399   acc: 100.00 | Test loss: 2.7670  acc: 100.00\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  1.46it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch 5] Train loss: 1.6405   acc: 100.00 | Test loss: 2.7675  acc: 100.00\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  1.47it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch 6] Train loss: 1.6311   acc: 100.00 | Test loss: 2.7680  acc: 100.00\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  1.49it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch 7] Train loss: 1.6214   acc: 100.00 | Test loss: 2.7684  acc: 100.00\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  1.48it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch 8] Train loss: 1.6075   acc: 100.00 | Test loss: 2.7687  acc: 100.00\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  1.46it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch 9] Train loss: 1.5987   acc: 100.00 | Test loss: 2.7690  acc: 100.00\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  1.45it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch 10] Train loss: 1.5939   acc: 100.00 | Test loss: 2.7692  acc: 100.00\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  1.47it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch 11] Train loss: 1.5998   acc: 100.00 | Test loss: 2.7694  acc: 100.00\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  1.45it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch 12] Train loss: 1.5997   acc: 100.00 | Test loss: 2.7696  acc: 100.00\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  1.45it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch 13] Train loss: 1.5835   acc: 100.00 | Test loss: 2.7698  acc: 100.00\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  1.47it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch 14] Train loss: 1.5941   acc: 100.00 | Test loss: 2.7699  acc: 100.00\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  1.41it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch 15] Train loss: 1.5962   acc: 100.00 | Test loss: 2.7700  acc: 100.00\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  1.43it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch 16] Train loss: 1.5974   acc: 100.00 | Test loss: 2.7701  acc: 100.00\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  1.47it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch 17] Train loss: 1.6057   acc: 100.00 | Test loss: 2.7701  acc: 100.00\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  1.46it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch 18] Train loss: 1.5824   acc: 100.00 | Test loss: 2.7701  acc: 100.00\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  1.46it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch 19] Train loss: 1.5678   acc: 100.00 | Test loss: 2.7701  acc: 100.00\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  1.41it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch 20] Train loss: 1.5796   acc: 100.00 | Test loss: 2.7701  acc: 100.00\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  1.47it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch 21] Train loss: 1.5892   acc: 100.00 | Test loss: 2.7700  acc: 100.00\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  1.43it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch 22] Train loss: 1.5883   acc: 100.00 | Test loss: 2.7699  acc: 100.00\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  1.42it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch 23] Train loss: 1.5779   acc: 100.00 | Test loss: 2.7698  acc: 100.00\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  1.44it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch 24] Train loss: 1.5814   acc: 100.00 | Test loss: 2.7696  acc: 100.00\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  1.40it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch 25] Train loss: 1.5883   acc: 100.00 | Test loss: 2.7694  acc: 100.00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytz64QU1EzcE",
        "colab_type": "text"
      },
      "source": [
        "## 7. Start evaluation phase"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVjZ1JLfi-m2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "capnet.load_state_dict(torch.load(os.path.join(os.getcwd(),'capsule_' + str(21) + '.pt')))\n",
        "capnet.eval()\n",
        "\n",
        "tol_label = np.array([], dtype=np.float)\n",
        "tol_pred = np.array([], dtype=np.float)\n",
        "tol_pred_prob = np.array([], dtype=np.float)\n",
        "\n",
        "count = 0\n",
        "loss_test = 0\n",
        "\n",
        "for img_data, labels_data in tqdm(dataloader_test):\n",
        "\n",
        "        labels_data[labels_data > 1] = 1\n",
        "        img_label = labels_data.numpy().astype(np.float)\n",
        "\n",
        "        if gpu_id >= 0:\n",
        "          img_data = img_data.cuda(gpu_id)\n",
        "          labels_data = labels_data.cuda(gpu_id)\n",
        "\n",
        "        input_v = Variable(img_data)\n",
        "\n",
        "        x = vgg_ext(input_v)\n",
        "        classes, class_ = capnet(x, random=random_setting)\n",
        "\n",
        "        output_dis = class_.data.cpu()\n",
        "        output_pred = np.zeros((output_dis.shape[0]), dtype=np.float)\n",
        "\n",
        "        for i in range(output_dis.shape[0]):\n",
        "            if output_dis[i,1] >= output_dis[i,0]:\n",
        "                output_pred[i] = 1.0\n",
        "            else:\n",
        "                output_pred[i] = 0.0\n",
        "\n",
        "        tol_label = np.concatenate((tol_label, img_label))\n",
        "        tol_pred = np.concatenate((tol_pred, output_pred))\n",
        "        \n",
        "        pred_prob = torch.softmax(output_dis, dim=1)\n",
        "        tol_pred_prob = np.concatenate((tol_pred_prob, pred_prob[:,1].data.numpy()))\n",
        "\n",
        "        count += 1\n",
        "\n",
        "acc_test = metrics.accuracy_score(tol_label, tol_pred)\n",
        "loss_test /= count\n",
        "\n",
        "fpr, tpr, thresholds = roc_curve(tol_label, tol_pred_prob, pos_label=1)\n",
        "eer = brentq(lambda x : 1. - x - interp1d(fpr, tpr)(x), 0., 1.)\n",
        "\n",
        "# fnr = 1 - tpr\n",
        "# hter = (fpr + fnr)/2\n",
        "\n",
        "print('[Epoch %d] Test acc: %.2f   EER: %.2f' % (21, acc_test*100, eer*100))\n",
        "text_writer_test.write('%d,%.2f,%.2f\\n'% (21, acc_test*100, eer*100))\n",
        "\n",
        "text_writer_test.flush()\n",
        "text_writer_test.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mj8hxuMgWhpe",
        "colab_type": "text"
      },
      "source": [
        "# Train ClassNSeg model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fvKFyADBmwvM",
        "colab_type": "text"
      },
      "source": [
        "**Public repository:** \n",
        "https://github.com/nii-yamagishilab/ClassNSeg \n",
        "\n",
        "**Reference:**\n",
        "H. H. Nguyen, F. Fang, J. Yamagishi, and I. Echizen, “Multi-task Learning for Detecting and Segmenting Manipulated Facial Images and Videos,” Proc. of the 10th IEEE International Conference on Biometrics: Theory, Applications and Systems (BTAS), 8 pages, (September 2019)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vewK3OwGoESG",
        "colab_type": "text"
      },
      "source": [
        "## 1. Import dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHKeQVSwWmTX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.backends.cudnn as cudnn\n",
        "from torch import nn\n",
        "from torch.optim import Adam\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "from tqdm import tqdm\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "import random"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1ieNfPNoGGp",
        "colab_type": "text"
      },
      "source": [
        "## 2. ClassNSeg Network architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdQHFvXEWrBE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, depth=3):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(depth, 8, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(8),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.Conv2d(8, 8, kernel_size=3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(8),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.Conv2d(8, 16, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.Conv2d(32, 32, kernel_size=3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        self.encoder.apply(self.weights_init)\n",
        "\n",
        "    def weights_init(self, m):\n",
        "        classname = m.__class__.__name__\n",
        "        if classname.find('Conv') != -1:\n",
        "            m.weight.data.normal_(0.0, 0.02)\n",
        "        elif classname.find('BatchNorm') != -1:\n",
        "            m.weight.data.normal_(0.5, 0.02)\n",
        "            m.bias.data.fill_(0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.encoder(x)\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, depth=3):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.shared = nn.Sequential(\n",
        "            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=1, padding=1, output_padding=0),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.ConvTranspose2d(64, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=1, padding=1, output_padding=0),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.ConvTranspose2d(32, 32, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        self.segmenter = nn.Sequential(\n",
        "\n",
        "            nn.ConvTranspose2d(32, 16, kernel_size=3, stride=1, padding=1, output_padding=0),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.ConvTranspose2d(16, 16, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.ConvTranspose2d(16, 8, kernel_size=3, stride=1, padding=1, output_padding=0),\n",
        "            nn.BatchNorm2d(8),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.ConvTranspose2d(8, 8, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
        "            nn.BatchNorm2d(8),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.ConvTranspose2d(8, 2, kernel_size=3, stride=1, padding=1, output_padding=0),\n",
        "            nn.Softmax(dim=1)\n",
        "        )\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "\n",
        "            nn.ConvTranspose2d(32, 16, kernel_size=3, stride=1, padding=1, output_padding=0),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.ConvTranspose2d(16, 16, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.ConvTranspose2d(16, 8, kernel_size=3, stride=1, padding=1, output_padding=0),\n",
        "            nn.BatchNorm2d(8),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.ConvTranspose2d(8, 8, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
        "            nn.BatchNorm2d(8),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.ConvTranspose2d(8, depth, kernel_size=3, stride=1, padding=1, output_padding=0),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "        self.segmenter.apply(self.weights_init)\n",
        "        self.decoder.apply(self.weights_init)\n",
        "\n",
        "    def weights_init(self, m):\n",
        "        classname = m.__class__.__name__\n",
        "        if classname.find('Conv') != -1:\n",
        "            m.weight.data.normal_(0.0, 0.02)\n",
        "        elif classname.find('BatchNorm') != -1:\n",
        "            m.weight.data.normal_(0.5, 0.02)\n",
        "            m.bias.data.fill_(0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        latent = self.shared(x)\n",
        "        seg = self.segmenter(latent)\n",
        "        rect = self.decoder(latent)\n",
        "\n",
        "        return seg, rect\n",
        "\n",
        "class ActivationLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ActivationLoss, self).__init__()\n",
        "\n",
        "    def forward(self, zero, one, labels):\n",
        "\n",
        "        loss_act = torch.abs(one - labels.data) + torch.abs(zero - (1.0 - labels.data))\n",
        "        return 1 / labels.shape[0] * loss_act.sum()\n",
        "        \n",
        "class ReconstructionLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ReconstructionLoss, self).__init__()\n",
        "        self.loss = nn.MSELoss()\n",
        "\n",
        "    def forward(self, reconstruction, groundtruth):\n",
        "\n",
        "        return self.loss(reconstruction, groundtruth.data)\n",
        "\n",
        "class SegmentationLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SegmentationLoss, self).__init__()\n",
        "        self.loss = nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, segment, groundtruth):\n",
        "\n",
        "        return self.loss(segment.view(segment.shape[0], segment.shape[1], segment.shape[2] * segment.shape[3]), \n",
        "            groundtruth.data.view(groundtruth.shape[0], groundtruth.shape[1] * groundtruth.shape[2]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15sywLwhoVVb",
        "colab_type": "text"
      },
      "source": [
        "## 3. Default settings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bEQ2FN3gE4fI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "367807a1-4fd8-436f-ab35-f91f6258f927"
      },
      "source": [
        "dataset = '/celeb-df'\n",
        "train_set = '/train'\n",
        "val_set = '/validation'\n",
        "test_set = '/test'\n",
        "workers = 0\n",
        "batch_size = 64\n",
        "manual_seed = random.randint(1, 1000)\n",
        "print(\"Random Seed: \", manual_seed)\n",
        "random.seed(manual_seed)\n",
        "torch.manual_seed(manual_seed)\n",
        "torch.cuda.manual_seed_all(manual_seed)\n",
        "cudnn.benchmark = True\n",
        "learning_rate = 0.001\n",
        "beta1_for_adam = 0.9\n",
        "weight_decay = 0.0005\n",
        "eps = 1e-07\n",
        "gpu_id = 0\n",
        "gamma = 1\n",
        "text_writer_train = open(os.path.join(os.getcwd(), 'train.csv'), 'a')\n",
        "text_writer_test = open(os.path.join(os.getcwd(), 'classification.txt'), 'w')"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Random Seed:  115\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3JlxE0xobTQ",
        "colab_type": "text"
      },
      "source": [
        "## 4. Creating ClassNSeg"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkxrZ24codA1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "53382796-bc26-4301-b91a-ab82e6586fa1"
      },
      "source": [
        "encoder = Encoder(3)\n",
        "decoder = Decoder(3)\n",
        "act_loss_fn = ActivationLoss()\n",
        "rect_loss_fn = ReconstructionLoss()\n",
        "seg_loss_fn = SegmentationLoss()\n",
        "\n",
        "optimizer_encoder = Adam(encoder.parameters(), lr=learning_rate, betas=(beta1_for_adam, 0.999), weight_decay=weight_decay, eps=eps)\n",
        "optimizer_decoder = Adam(decoder.parameters(), lr=learning_rate, betas=(beta1_for_adam, 0.999), weight_decay=weight_decay, eps=eps)\n",
        "\n",
        "encoder.cuda(gpu_id)\n",
        "decoder.cuda(gpu_id)\n",
        "act_loss_fn.cuda(gpu_id)\n",
        "seg_loss_fn.cuda(gpu_id)\n",
        "rect_loss_fn.cuda(gpu_id)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ReconstructionLoss(\n",
              "  (loss): MSELoss()\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGl_QbmMonwA",
        "colab_type": "text"
      },
      "source": [
        "## 5. Creating datasets for training, validation, and test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RImo2nnSoov2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        },
        "outputId": "d00c9688-c9d2-4636-e9f0-6b76d6da0816"
      },
      "source": [
        "class Normalize_3D(object):\n",
        "        def __init__(self, mean, std):\n",
        "            self.mean = mean\n",
        "            self.std = std\n",
        "\n",
        "        def __call__(self, tensor):\n",
        "            \"\"\"\n",
        "                Tensor: Normalized image.\n",
        "            Args:\n",
        "                tensor (Tensor): Tensor image of size (C, H, W) to be normalized.\n",
        "            Returns:        \"\"\"\n",
        "            for t, m, s in zip(tensor, self.mean, self.std):\n",
        "                t.sub_(m).div_(s)\n",
        "            return tensor\n",
        "\n",
        "class UnNormalize_3D(object):\n",
        "        def __init__(self, mean, std):\n",
        "            self.mean = mean\n",
        "            self.std = std\n",
        "\n",
        "        def __call__(self, tensor):\n",
        "            \"\"\"\n",
        "                Tensor: Normalized image.\n",
        "            Args:\n",
        "                tensor (Tensor): Tensor image of size (C, H, W) to be normalized.\n",
        "            Returns:        \"\"\"\n",
        "            for t, m, s in zip(tensor, self.mean, self.std):\n",
        "                t.mul_(s).add_(m)\n",
        "            return tensor\n",
        "\n",
        "transform_tns = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "\n",
        "transform_pil = transforms.Compose([\n",
        "        transforms.ToPILImage(),\n",
        "    ])\n",
        "    \n",
        "transform_norm = Normalize_3D((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
        "\n",
        "transform_unnorm = UnNormalize_3D((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
        "\n",
        "dataset_train = dset.ImageFolder(root=os.path.join(dataset, train_set), transform=transform_tns)\n",
        "assert dataset_train\n",
        "dataloader_train = torch.utils.data.DataLoader(dataset_train, batch_size=batch_size, shuffle=True, num_workers=int(workers))\n",
        "\n",
        "dataset_val = dset.ImageFolder(root=os.path.join(dataset, val_set), transform=transform_tns)\n",
        "assert dataset_val\n",
        "dataloader_val = torch.utils.data.DataLoader(dataset_val, batch_size=batch_size, shuffle=False, num_workers=int(workers))\n",
        "\n",
        "dataset_test = dset.ImageFolder(root=os.path.join(dataset, test_set), transform=transform_tns)\n",
        "assert dataset_test\n",
        "dataloader_test = torch.utils.data.DataLoader(dataset_test, batch_size=batch_size, shuffle=False, num_workers=int(workers))"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-2aa4938d98d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0mtransform_unnorm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUnNormalize_3D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.485\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.456\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.406\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0.229\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.225\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0mdataset_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform_tns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mdataset_train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0mdataloader_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file)\u001b[0m\n\u001b[1;32m    207\u001b[0m                                           \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m                                           \u001b[0mtarget_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m                                           is_valid_file=is_valid_file)\n\u001b[0m\u001b[1;32m    210\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001b[0m\n\u001b[1;32m     91\u001b[0m         super(DatasetFolder, self).__init__(root, transform=transform,\n\u001b[1;32m     92\u001b[0m                                             target_transform=target_transform)\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_find_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m         \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextensions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_valid_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m_find_classes\u001b[0;34m(self, dir)\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# Faster and available in Python 3.5 and above\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/train'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTZxQ_9cE0Yf",
        "colab_type": "text"
      },
      "source": [
        "## 6. Start pre-processing phase"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUriqyjVXLnI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_real = '/celeb-df/'\n",
        "input_fake = ''\n",
        "mask = ''\n",
        "output_real = 'deepfakes/real'\n",
        "output_fake = 'deepfakes/fake'\n",
        "image_size = 256\n",
        "limit = 10\n",
        "scale = 1.3\n",
        "\n",
        "def to_bw(mask, thresh_binary=10, thresh_otsu=255):\n",
        "    im_gray = cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY)\n",
        "    (thresh, im_bw) = cv2.threshold(im_gray, thresh_binary, thresh_otsu, cv2.THRESH_BINARY | cv2.THRESH_OTSU)\n",
        "\n",
        "    return im_bw\n",
        "\n",
        "def get_bbox(mask, thresh_binary=127, thresh_otsu=255):\n",
        "    im_bw = to_bw(mask, thresh_binary, thresh_otsu)\n",
        "\n",
        "    # im2, contours, hierarchy = cv2.findContours(im_bw,cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)\n",
        "    contours, hierarchy = cv2.findContours(im_bw,cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "    locations = np.array([], dtype=np.int).reshape(0, 5)\n",
        "\n",
        "    for c in contours:\n",
        "        # calculate moments for each contour\n",
        "        M = cv2.moments(c)\n",
        "        if M[\"m00\"] > 0:\n",
        "            cX = int(M[\"m10\"] / M[\"m00\"])\n",
        "        else:\n",
        "            cX = 0\n",
        "        if M[\"m00\"] > 0:\n",
        "            cY = int(M[\"m01\"] / M[\"m00\"])\n",
        "        else:\n",
        "            cY = 0\n",
        "\n",
        "        # calculate the rectangle bounding box\n",
        "        x,y,w,h = cv2.boundingRect(c)\n",
        "        locations = np.concatenate((locations, np.array([[cX, cY, w, h, w + h]])), axis=0)\n",
        "\n",
        "    max_idex = locations[:,4].argmax()\n",
        "    bbox = locations[max_idex, 0:4].reshape(4)\n",
        "    return bbox\n",
        "\n",
        "def extract_face(image, bbox, scale = 1.0):\n",
        "    h, w, d = image.shape\n",
        "    radius = int(bbox[3] * scale / 2)\n",
        "\n",
        "    y_1 = bbox[1] - radius\n",
        "    y_2 = bbox[1] + radius\n",
        "    x_1 = bbox[0] - radius\n",
        "    x_2 = bbox[0] + radius\n",
        "\n",
        "    if x_1 < 0:\n",
        "        x_1 = 0\n",
        "    if y_1 < 0:\n",
        "        y_1 = 0\n",
        "    if x_2 > w:\n",
        "        x_2 = w\n",
        "    if y_2 > h:\n",
        "        y_2 = h\n",
        "\n",
        "    crop_img = image[y_1:y_2, x_1:x_2]\n",
        "\n",
        "    if crop_img is not None:\n",
        "        crop_img = cv2.resize(crop_img, (image_size, image_size))\n",
        "\n",
        "    return crop_img\n",
        "\n",
        "def extract_face_videos(input_real, input_fake, input_mask, output_real, output_fake):\n",
        "\n",
        "    blank_img = np.zeros((image_size,image_size,3), np.uint8)\n",
        "\n",
        "    for f in os.listdir(input_fake):\n",
        "        if os.path.isfile(os.path.join(input_fake, f)):\n",
        "            if f.lower().endswith(('mp4')):\n",
        "                print(f)\n",
        "                filename = os.path.splitext(f)[0]\n",
        "\n",
        "                vidcap_real = cv2.VideoCapture(os.path.join(input_real, filename[0:3] + '.mp4'))\n",
        "                success_real, image_real = vidcap_real.read()\n",
        "\n",
        "                vidcap_fake = cv2.VideoCapture(os.path.join(input_fake, f))\n",
        "                success_fake, image_fake = vidcap_fake.read()\n",
        "\n",
        "                image_mask = cv2.imread(os.path.join(input_mask, filename, '0000.png'))\n",
        "\n",
        "                count = 0\n",
        "\n",
        "                while (success_real and success_fake):\n",
        "\n",
        "                    bbox = get_bbox(image_mask)\n",
        "\n",
        "                    if bbox is None:\n",
        "                        count += 1\n",
        "                        continue\n",
        "\n",
        "                    original_cropped = extract_face(image_real, bbox, scale)\n",
        "                    altered_cropped = extract_face(image_fake, bbox, scale)\n",
        "\n",
        "                    mask_cropped = to_bw(extract_face(image_mask, bbox, scale))\n",
        "                    mask_cropped = np.stack((mask_cropped,mask_cropped, mask_cropped), axis=2)\n",
        "\n",
        "                    if (original_cropped is not None) and (altered_cropped is not None) and (mask_cropped is not None):\n",
        "                        original_cropped = np.concatenate((original_cropped, blank_img), axis=1)\n",
        "                        altered_cropped = np.concatenate((altered_cropped, mask_cropped), axis=1)\n",
        "\n",
        "                        cv2.imwrite(os.path.join(output_real, filename + \"_%d.jpg\" % count), original_cropped)\n",
        "                        cv2.imwrite(os.path.join(output_fake, filename + \"_%d.jpg\" % count), altered_cropped)\n",
        "\n",
        "                        count += 1\n",
        "\n",
        "                    if count >= opt.limit:\n",
        "                        break\n",
        "\n",
        "                    success_real, image_real = vidcap_real.read()\n",
        "                    success_fake, image_fake = vidcap_fake.read()\n",
        "                    image_mask = cv2.imread(os.path.join(input_mask, filename, str(count).zfill(4) + '.png'))\n",
        "\n",
        "extract_face_videos(input_real, input_fake, mask, output_real, output_fake)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fZT8kFvory0",
        "colab_type": "text"
      },
      "source": [
        "## 7. Start training phase\n",
        "- Save the fitted model at the end"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ool-TzrjovJW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254
        },
        "outputId": "7759aecb-1cf6-4bfb-883e-635e3a47ec09"
      },
      "source": [
        "for epoch in range(1, 101):\n",
        "        count = 0\n",
        "        loss_act_train = 0.0\n",
        "        loss_seg_train = 0.0\n",
        "        loss_rect_train = 0.0\n",
        "        loss_act_test = 0.0\n",
        "        loss_seg_test = 0.0\n",
        "        loss_rect_test = 0.0\n",
        "\n",
        "        tol_label = np.array([], dtype=np.float)\n",
        "        tol_pred = np.array([], dtype=np.float)\n",
        "\n",
        "        for fft_data, labels_data in tqdm(dataloader_train):\n",
        "\n",
        "            optimizer_encoder.zero_grad()\n",
        "            optimizer_decoder.zero_grad()\n",
        "\n",
        "            fft_label = labels_data.numpy().astype(np.float)\n",
        "            labels_data = labels_data.float()\n",
        "\n",
        "            rgb = transform_norm(fft_data[:,:,:,0:256])\n",
        "            mask = fft_data[:,0,:,256:512]\n",
        "            mask[mask >= 0.5] = 1.0\n",
        "            mask[mask < 0.5] = 0.0\n",
        "            mask = mask.long()\n",
        "\n",
        "            if gpu_id >= 0:\n",
        "              rgb = rgb.cuda(gpu_id)\n",
        "              mask = mask.cuda(gpu_id)\n",
        "              labels_data = labels_data.cuda(gpu_id)\n",
        "\n",
        "            latent = encoder(rgb).reshape(-1, 2, 64, 16, 16)\n",
        "\n",
        "            zero_abs = torch.abs(latent[:,0]).view(latent.shape[0], -1)\n",
        "            zero = zero_abs.mean(dim=1)\n",
        "\n",
        "            one_abs = torch.abs(latent[:,1]).view(latent.shape[0], -1)\n",
        "            one = one_abs.mean(dim=1)\n",
        "\n",
        "            loss_act = act_loss_fn(zero, one, labels_data)\n",
        "            loss_act_data = loss_act.item()\n",
        "\n",
        "            y = torch.eye(2)\n",
        "            if gpu_id >= 0:\n",
        "              y = y.cuda(gpu_id)\n",
        "\n",
        "            y = y.index_select(dim=0, index=labels_data.data.long())\n",
        "\n",
        "            latent = (latent * y[:,:,None, None, None]).reshape(-1, 128, 16, 16)\n",
        "\n",
        "            seg, rect = decoder(latent)\n",
        "\n",
        "            loss_seg = seg_loss_fn(seg, mask)\n",
        "            loss_seg = loss_seg * gamma\n",
        "            loss_seg_data = loss_seg.item()\n",
        "\n",
        "            loss_rect = rect_loss_fn(rect, rgb)\n",
        "            loss_rect = loss_rect * gamma\n",
        "            loss_rect_data = loss_rect.item()\n",
        "\n",
        "            loss_total = loss_act + loss_seg + loss_rect\n",
        "            loss_total.backward()\n",
        "\n",
        "            optimizer_decoder.step()\n",
        "            optimizer_encoder.step()\n",
        "\n",
        "            output_pred = np.zeros((fft_data.shape[0]), dtype=np.float)\n",
        "\n",
        "            for i in range(fft_data.shape[0]):\n",
        "                if one[i] >= zero[i]:\n",
        "                    output_pred[i] = 1.0\n",
        "                else:\n",
        "                    output_pred[i] = 0.0\n",
        "\n",
        "            tol_label = np.concatenate((tol_label, fft_label))\n",
        "            tol_pred = np.concatenate((tol_pred, output_pred))\n",
        "\n",
        "            loss_act_train += loss_act_data\n",
        "            loss_seg_train += loss_seg_data\n",
        "            loss_rect_train += loss_rect_data\n",
        "            count += 1\n",
        "\n",
        "        acc_train = metrics.accuracy_score(tol_label, tol_pred)\n",
        "        loss_act_train /= count\n",
        "        loss_seg_train /= count\n",
        "        loss_rect_train /= count\n",
        "\n",
        "        ########################################################################\n",
        "        # do checkpointing & validation\n",
        "\n",
        "        torch.save(encoder.state_dict(), os.path.join(os.getcwd(), 'encoder_%d.pt' % epoch))\n",
        "        torch.save(optimizer_encoder.state_dict(), os.path.join(os.getcwd(), 'optim_encoder_%d.pt' % epoch))\n",
        "\n",
        "        torch.save(decoder.state_dict(), os.path.join(os.getcwd(), 'decoder_%d.pt' % epoch))\n",
        "        torch.save(optimizer_decoder.state_dict(), os.path.join(os.getcwd(), 'optim_decoder_%d.pt' % epoch))\n",
        "\n",
        "        encoder.eval()\n",
        "        decoder.eval()\n",
        "\n",
        "        tol_label = np.array([], dtype=np.float)\n",
        "        tol_pred = np.array([], dtype=np.float)\n",
        "        tol_pred_prob = np.array([], dtype=np.float)\n",
        "\n",
        "        count = 0\n",
        "\n",
        "        for fft_data, labels_data in tqdm(dataloader_val):\n",
        "\n",
        "            fft_label = labels_data.numpy().astype(np.float)\n",
        "            labels_data = labels_data.float()\n",
        "\n",
        "            rgb = transform_norm(fft_data[:,:,:,0:256])\n",
        "            mask = fft_data[:,0,:,256:512]\n",
        "            mask[mask >= 0.5] = 1.0\n",
        "            mask[mask < 0.5] = 0.0\n",
        "            mask = mask.long()\n",
        "\n",
        "            if gpu_id >= 0:\n",
        "                rgb = rgb.cuda(gpu_id)\n",
        "                mask = mask.cuda(gpu_id)\n",
        "                labels_data = labels_data.cuda(gpu_id)\n",
        "\n",
        "            latent = encoder(rgb).reshape(-1, 2, 64, 16, 16)\n",
        "\n",
        "            zero_abs = torch.abs(latent[:,0]).view(latent.shape[0], -1)\n",
        "            zero = zero_abs.mean(dim=1)\n",
        "\n",
        "            one_abs = torch.abs(latent[:,1]).view(latent.shape[0], -1)\n",
        "            one = one_abs.mean(dim=1)\n",
        "\n",
        "            loss_act = act_loss_fn(zero, one, labels_data)\n",
        "            loss_act_data = loss_act.item()\n",
        "\n",
        "            y = torch.eye(2)\n",
        "            if gpu_id >= 0:\n",
        "                y = y.cuda(gpu_id)\n",
        "\n",
        "            y = y.index_select(dim=0, index=labels_data.data.long())\n",
        "\n",
        "            latent = (latent * y[:,:,None, None, None]).reshape(-1, 128, 16, 16)\n",
        "\n",
        "            seg, rect = decoder(latent)\n",
        "\n",
        "            loss_seg = seg_loss_fn(seg, mask)\n",
        "            loss_seg = loss_seg * gamma\n",
        "            loss_seg_data = loss_seg.item()\n",
        "\n",
        "            loss_rect = rect_loss_fn(rect, rgb)\n",
        "            loss_rect = loss_rect * gamma\n",
        "            loss_rect_data = loss_rect.item()\n",
        "\n",
        "            output_pred = np.zeros((fft_data.shape[0]), dtype=np.float)\n",
        "\n",
        "            for i in range(fft_data.shape[0]):\n",
        "                if one[i] >= zero[i]:\n",
        "                    output_pred[i] = 1.0\n",
        "                else:\n",
        "                    output_pred[i] = 0.0\n",
        "\n",
        "            tol_label = np.concatenate((tol_label, fft_label))\n",
        "            tol_pred = np.concatenate((tol_pred, output_pred))\n",
        "            \n",
        "            pred_prob = torch.softmax(torch.cat((zero.reshape(zero.shape[0],1), one.reshape(one.shape[0],1)), dim=1), dim=1)\n",
        "            tol_pred_prob = np.concatenate((tol_pred_prob, pred_prob[:,1].data.cpu().numpy()))\n",
        "\n",
        "            loss_act_test += loss_act_data\n",
        "            loss_seg_test += loss_seg_data\n",
        "            loss_rect_test += loss_rect_data\n",
        "            count += 1\n",
        "\n",
        "        acc_test = metrics.accuracy_score(tol_label, tol_pred)\n",
        "        loss_act_test /= count\n",
        "        loss_seg_test /= count\n",
        "        loss_rect_test /= count\n",
        "\n",
        "        fpr, tpr, thresholds = roc_curve(tol_label, tol_pred_prob, pos_label=1)\n",
        "        eer = brentq(lambda x : 1. - x - interp1d(fpr, tpr)(x), 0., 1.)\n",
        "\n",
        "        print('[Epoch %d] Train: act_loss: %.4f  seg_loss: %.4f  rect_loss: %.4f  acc: %.2f | Test: act_loss: %.4f  seg_loss: %.4f  rect_loss: %.4f  acc: %.2f  eer: %.2f'\n",
        "        % (epoch, loss_act_train, loss_seg_train, loss_rect_train, acc_train*100, loss_act_test, loss_seg_test, loss_rect_test, acc_test*100, eer*100))\n",
        "\n",
        "        text_writer.write('%d,%.4f,%.4f,%.4f,%.2f,%.4f,%.4f,%.4f,%.2f,%.2f\\n'\n",
        "        % (epoch, loss_act_train, loss_seg_train, loss_rect_train, acc_train*100, loss_act_test, loss_seg_test, loss_rect_test, acc_test*100, eer*100))\n",
        "\n",
        "        text_writer_train.flush()\n",
        "\n",
        "        ########################################################################\n",
        "\n",
        "        real_img = transform_tns(Image.open(os.path.join('test_img', 'real.jpg'))).unsqueeze(0)[:,:,:,0:256]\n",
        "        real_mask = transform_tns(Image.open(os.path.join('test_img', 'real.jpg'))).unsqueeze(0)[:,:,:,256:512]\n",
        "        fake_img = transform_tns(Image.open(os.path.join('test_img', 'fake.jpg'))).unsqueeze(0)[:,:,:,0:256]\n",
        "        fake_mask = transform_tns(Image.open(os.path.join('test_img', 'fake.jpg'))).unsqueeze(0)[:,:,:,256:512]\n",
        "\n",
        "        rgb = torch.cat((real_img, fake_img), dim=0)\n",
        "        rgb = transform_norm(rgb)\n",
        "\n",
        "        real_mask[real_mask >= 0.5] = 1.0\n",
        "        real_mask[real_mask < 0.5] = 0.0\n",
        "        real_mask = real_mask.long()\n",
        "\n",
        "        fake_mask[fake_mask >= 0.5] = 1.0\n",
        "        fake_mask[fake_mask < 0.5] = 0.0\n",
        "        fake_mask = fake_mask.long()\n",
        "\n",
        "        # real = 1, fake = 0\n",
        "        labels_data = torch.FloatTensor([1,0])\n",
        "\n",
        "        if gpu_id >= 0:\n",
        "          rgb = rgb.cuda(gpu_id)\n",
        "          labels_data = labels_data.cuda(gpu_id)\n",
        "\n",
        "        latent = encoder(rgb).reshape(-1, 2, 64, 16, 16)\n",
        "\n",
        "        zero_abs = torch.abs(latent[:,0]).view(latent.shape[0], -1)\n",
        "        zero = zero_abs.mean(dim=1)\n",
        "\n",
        "        one_abs = torch.abs(latent[:,1]).view(latent.shape[0], -1)\n",
        "        one = one_abs.mean(dim=1)\n",
        "\n",
        "        y = torch.eye(2)\n",
        "\n",
        "        if gpu_id >= 0:\n",
        "          y = y.cuda(gpu_id)\n",
        "\n",
        "        y = y.index_select(dim=0, index=labels_data.data.long())\n",
        "\n",
        "        latent = (latent * y[:,:,None, None, None]).reshape(-1, 128, 16, 16)\n",
        "\n",
        "        seg, rect = decoder(latent)\n",
        "\n",
        "        seg = seg[:,1,:,:].detach().cpu()\n",
        "        seg[seg >= 0.5] = 1.0\n",
        "        seg[seg < 0.5] = 0.0\n",
        "\n",
        "        rect = transform_unnorm(rect).detach().cpu()\n",
        "\n",
        "        real_seg = transform_pil(seg[0])\n",
        "        fake_seg = transform_pil(seg[1])\n",
        "\n",
        "        real_img = transform_pil(rect[0])\n",
        "        fake_img = transform_pil(rect[1])\n",
        "\n",
        "        real_seg.save(os.path.join(os.getcwd(), 'image', 'seg_real_' + str(epoch).zfill(3) + '.jpg'))\n",
        "        fake_seg.save(os.path.join(os.getcwd(), 'image', 'seg_fake_' + str(epoch).zfill(3) + '.jpg'))\n",
        "\n",
        "        real_img.save(os.path.join(os.getcwd(), 'image', 'real_' + str(epoch).zfill(3) + '.jpg'))\n",
        "        fake_img.save(os.path.join(os.getcwd(), 'image', 'fake_' + str(epoch).zfill(3) + '.jpg'))\n",
        "\n",
        "        encoder.train(mode=True)\n",
        "        decoder.train(mode=True)\n",
        "\n",
        "text_writer_train.close()"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-25b7a011009d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mtol_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mfft_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_data\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0moptimizer_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'dataloader_train' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0Yh5s1xpJlE",
        "colab_type": "text"
      },
      "source": [
        "## 8. Start evaluation phase"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z24BbUe6pL0r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254
        },
        "outputId": "ad5bba66-3567-4fe2-9931-9a6d820109f5"
      },
      "source": [
        "encoder.load_state_dict(torch.load(os.path.join(os.getcwd(),'encoder_' + str(46) + '.pt')))\n",
        "encoder.eval()\n",
        "\n",
        "loss_act_test = 0.0\n",
        "\n",
        "tol_label = np.array([], dtype=np.float)\n",
        "tol_pred = np.array([], dtype=np.float)\n",
        "tol_pred_prob = np.array([], dtype=np.float)\n",
        "\n",
        "count = 0\n",
        "\n",
        "for fft_data, labels_data in tqdm(dataloader_test):\n",
        "\n",
        "        fft_label = labels_data.numpy().astype(np.float)\n",
        "        labels_data = labels_data.float()\n",
        "\n",
        "        rgb = transform_norm(fft_data[:,:,:,0:256])\n",
        "\n",
        "        if gpu_id >= 0:\n",
        "          rgb = rgb.cuda(gpu_id)\n",
        "          labels_data = labels_data.cuda(gpu_id)\n",
        "\n",
        "        latent = encoder(rgb).reshape(-1, 2, 64, 16, 16)\n",
        "\n",
        "        zero_abs = torch.abs(latent[:,0]).view(latent.shape[0], -1)\n",
        "        zero = zero_abs.mean(dim=1)\n",
        "\n",
        "        one_abs = torch.abs(latent[:,1]).view(latent.shape[0], -1)\n",
        "        one = one_abs.mean(dim=1)\n",
        "\n",
        "        loss_act = act_loss_fn(zero, one, labels_data)\n",
        "        loss_act_data = loss_act.item()\n",
        "\n",
        "        output_pred = np.zeros((fft_data.shape[0]), dtype=np.float)\n",
        "\n",
        "        for i in range(fft_data.shape[0]):\n",
        "            if one[i] >= zero[i]:\n",
        "                output_pred[i] = 1.0\n",
        "            else:\n",
        "                output_pred[i] = 0.0\n",
        "\n",
        "        tol_label = np.concatenate((tol_label, fft_label))\n",
        "        tol_pred = np.concatenate((tol_pred, output_pred))\n",
        "        \n",
        "        pred_prob = torch.softmax(torch.cat((zero.reshape(zero.shape[0],1), one.reshape(one.shape[0],1)), dim=1), dim=1)\n",
        "        tol_pred_prob = np.concatenate((tol_pred_prob, pred_prob[:,1].data.cpu().numpy()))\n",
        "\n",
        "        loss_act_test += loss_act_data\n",
        "        count += 1\n",
        "\n",
        "acc_test = metrics.accuracy_score(tol_label, tol_pred)\n",
        "loss_act_test /= count\n",
        "\n",
        "fpr, tpr, thresholds = roc_curve(tol_label, tol_pred_prob, pos_label=1)\n",
        "eer = brentq(lambda x : 1. - x - interp1d(fpr, tpr)(x), 0., 1.)\n",
        "\n",
        "print('[Epoch %d] act_loss: %.4f  acc: %.2f   eer: %.2f' % (opt.id, loss_act_test, acc_test*100, eer*100))\n",
        "text_writer_test.write('%d,%.4f,%.2f,%.2f\\n'% (46, loss_act_test, acc_test*100, eer*100))\n",
        "\n",
        "text_writer_test.flush()\n",
        "text_writer_test.close()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-b392017033a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'encoder_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m46\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mloss_act_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'encoder' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kS27O-NIX7kZ",
        "colab_type": "text"
      },
      "source": [
        "# Train DSP-FWA model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFVOvZaSZADN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torchvision import models\n",
        "import torch.nn.functional as F\n",
        "import os, math"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zJ8n4EzZGjP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ResNet(nn.Module):\n",
        "    def __init__(self, layers=18, num_class=2, pretrained=True):\n",
        "        super(ResNet, self).__init__()\n",
        "        if layers == 18:\n",
        "            self.resnet = models.resnet18(pretrained=pretrained)\n",
        "        elif layers == 34:\n",
        "            self.resnet = models.resnet34(pretrained=pretrained)\n",
        "        elif layers == 50:\n",
        "            self.resnet = models.resnet50(pretrained=pretrained)\n",
        "        elif layers == 101:\n",
        "            self.resnet = models.resnet101(pretrained=pretrained)\n",
        "        elif layers == 152:\n",
        "            self.resnet = models.resnet152(pretrained=pretrained)\n",
        "        else:\n",
        "            raise ValueError('layers should be 18, 34, 50, 101.')\n",
        "        self.num_class = num_class\n",
        "        if layers in [18, 34]:\n",
        "            self.fc = nn.Linear(512, num_class)\n",
        "        if layers in [50, 101, 152]:\n",
        "            self.fc = nn.Linear(512 * 4, num_class)\n",
        "\n",
        "    def conv_base(self, x):\n",
        "        x = self.resnet.conv1(x)\n",
        "        x = self.resnet.bn1(x)\n",
        "        x = self.resnet.relu(x)\n",
        "        x = self.resnet.maxpool(x)\n",
        "\n",
        "        layer1 = self.resnet.layer1(x)\n",
        "        layer2 = self.resnet.layer2(layer1)\n",
        "        layer3 = self.resnet.layer3(layer2)\n",
        "        layer4 = self.resnet.layer4(layer3)\n",
        "        return layer1, layer2, layer3, layer4\n",
        "\n",
        "    def forward(self, x):\n",
        "        layer1, layer2, layer3, layer4 = self.conv_base(x)\n",
        "        x = self.resnet.avgpool(layer4)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "class SPPNet(nn.Module):\n",
        "    def __init__(self, backbone=101, num_class=2, pool_size=(1, 2, 6), pretrained=True):\n",
        "        # Only resnet is supported in this version\n",
        "        super(SPPNet, self).__init__()\n",
        "        if backbone in [18, 34, 50, 101, 152]:\n",
        "            self.resnet = ResNet(backbone, num_class, pretrained)\n",
        "        else:\n",
        "            raise ValueError('Resnet{} is not supported yet.'.format(backbone))\n",
        "\n",
        "        if backbone in [18, 34]:\n",
        "            self.c = 512\n",
        "        if backbone in [50, 101, 152]:\n",
        "            self.c = 2048\n",
        "\n",
        "        self.spp = SpatialPyramidPool2D(out_side=pool_size)\n",
        "        num_features = self.c * (pool_size[0] ** 2 + pool_size[1] ** 2 + pool_size[2] ** 2)\n",
        "        self.classifier = nn.Linear(num_features, num_class)\n",
        "\n",
        "    def forward(self, x):\n",
        "        _, _, _, x = self.resnet.conv_base(x)\n",
        "        x = self.spp(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class SpatialPyramidPool2D(nn.Module):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        out_side (tuple): Length of side in the pooling results of each pyramid layer.\n",
        "\n",
        "    Inputs:\n",
        "        - `input`: the input Tensor to invert ([batch, channel, width, height])\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, out_side):\n",
        "        super(SpatialPyramidPool2D, self).__init__()\n",
        "        self.out_side = out_side\n",
        "\n",
        "    def forward(self, x):\n",
        "        # batch_size, c, h, w = x.size()\n",
        "        out = None\n",
        "        for n in self.out_side:\n",
        "            w_r, h_r = map(lambda s: math.ceil(s / n), x.size()[2:])  # Receptive Field Size\n",
        "            s_w, s_h = map(lambda s: math.floor(s / n), x.size()[2:])  # Stride\n",
        "            max_pool = nn.MaxPool2d(kernel_size=(w_r, h_r), stride=(s_w, s_h))\n",
        "            y = max_pool(x)\n",
        "            if out is None:\n",
        "                out = y.view(y.size()[0], -1)\n",
        "            else:\n",
        "                out = torch.cat((out, y.view(y.size()[0], -1)), 1)\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KaEVoG_NYAFW",
        "colab_type": "text"
      },
      "source": [
        "# Train Ictu Oculi model"
      ]
    }
  ]
}