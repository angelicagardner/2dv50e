{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "test_individual_models.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "AAphn-KTcYRT",
        "raLqusqc-HUO",
        "PI5gU66udZ9z",
        "FxK-zulhAabC",
        "1_b5xxMz-Siy",
        "VUKuV589-0Dz",
        "5QB1JiORc-_J",
        "6agcWVSsdYA6",
        "FS1vY39jddg-",
        "ytz64QU1EzcE",
        "Mj8hxuMgWhpe",
        "vewK3OwGoESG",
        "F1ieNfPNoGGp",
        "15sywLwhoVVb",
        "S3JlxE0xobTQ",
        "KTZxQ_9cE0Yf",
        "UGl_QbmMonwA",
        "6fZT8kFvory0",
        "j0Yh5s1xpJlE",
        "m9D96Qra_eQX",
        "ebaRRrpS_iTR",
        "pVw0i5nHBDs9",
        "MtOdbT3qCwS9",
        "KaEVoG_NYAFW",
        "eo_aDQ--IxLM",
        "Ln8fg8N8LD9M",
        "VZUzXHa2LHLz",
        "E2JGepy_PdYU"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3a5bb1c2e10847cbb04e6a703bf17c28": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_0ba4c5f24a0c4b97ae96229228a0daea",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_0c10db0e76a34c9c810550cc2ce80a4d",
              "IPY_MODEL_210d93ccd52c46a28e258fb04be39c19"
            ]
          }
        },
        "0ba4c5f24a0c4b97ae96229228a0daea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0c10db0e76a34c9c810550cc2ce80a4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_8db5187646864526bdaa281297a98236",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 574673361,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 574673361,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_daad4ffa3edc4f168c47f07c01af2138"
          }
        },
        "210d93ccd52c46a28e258fb04be39c19": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_3c3d467680a24822bbea50937d98c903",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 548M/548M [00:10&lt;00:00, 52.6MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_13a626f1f9f747b5b727982b921d223c"
          }
        },
        "8db5187646864526bdaa281297a98236": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "daad4ffa3edc4f168c47f07c01af2138": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3c3d467680a24822bbea50937d98c903": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "13a626f1f9f747b5b727982b921d223c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3XbIHhY-BUO1",
        "colab_type": "text"
      },
      "source": [
        "# Instructions when using Google Colab and Google Cloud Storage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AAphn-KTcYRT",
        "colab_type": "text"
      },
      "source": [
        "## Authenticate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jt47JXK7qSoP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNMUd8p5c1ia",
        "colab_type": "text"
      },
      "source": [
        "## Load data from Cloud Storage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "raLqusqc-HUO",
        "colab_type": "text"
      },
      "source": [
        "### Install Cloud Storage FUSE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRoR9kM0yi_3",
        "colab_type": "code",
        "outputId": "d1b4371f-fad3-4f53-f25e-dca408ce9ef3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        }
      },
      "source": [
        "!echo \"deb http://packages.cloud.google.com/apt gcsfuse-bionic main\" > /etc/apt/sources.list.d/gcsfuse.list\n",
        "!curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add -\n",
        "!apt -qq update\n",
        "!apt -qq install gcsfuse"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100   653  100   653    0     0   9069      0 --:--:-- --:--:-- --:--:--  9069\n",
            "OK\n",
            "54 packages can be upgraded. Run 'apt list --upgradable' to see them.\n",
            "The following NEW packages will be installed:\n",
            "  gcsfuse\n",
            "0 upgraded, 1 newly installed, 0 to remove and 54 not upgraded.\n",
            "Need to get 4,274 kB of archives.\n",
            "After this operation, 12.8 MB of additional disk space will be used.\n",
            "Selecting previously unselected package gcsfuse.\n",
            "(Reading database ... 144568 files and directories currently installed.)\n",
            "Preparing to unpack .../gcsfuse_0.28.1_amd64.deb ...\n",
            "Unpacking gcsfuse (0.28.1) ...\n",
            "Setting up gcsfuse (0.28.1) ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozMhFVbJ-axW",
        "colab_type": "text"
      },
      "source": [
        "### Create connection between Cloud Storage and Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCIJtvHBzIAx",
        "colab_type": "code",
        "outputId": "da16b301-471c-4d10-bcc6-349612cd2c1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        }
      },
      "source": [
        "# Connecting with dataset buckets\n",
        "!mkdir data\n",
        "!mkdir data/celeb-df\n",
        "!gcsfuse celeb-df data/celeb-df\n",
        "!ls data/celeb-df | wc\n",
        "\n",
        "# Connecting with checkpoints for the models\n",
        "!mkdir checkpoints\n",
        "!gcsfuse checkpoints_models checkpoints\n",
        "!ls checkpoints | wc"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘data’: File exists\n",
            "mkdir: cannot create directory ‘data/celeb-df’: File exists\n",
            "Using mount point: /content/data/celeb-df\n",
            "Opening GCS connection...\n",
            "Opening bucket...\n",
            "Mounting file system...\n",
            "File system has been successfully mounted.\n",
            "      3       3      22\n",
            "mkdir: cannot create directory ‘checkpoints’: File exists\n",
            "Using mount point: /content/checkpoints\n",
            "Opening GCS connection...\n",
            "Opening bucket...\n",
            "Mounting file system...\n",
            "File system has been successfully mounted.\n",
            "      2       2      18\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PI5gU66udZ9z",
        "colab_type": "text"
      },
      "source": [
        "# Train Capsule model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hod36IhHA4b9",
        "colab_type": "text"
      },
      "source": [
        "**Public repository:** \n",
        "https://github.com/nii-yamagishilab/Capsule-Forensics-v2\n",
        "\n",
        "**Reference:**\n",
        "H. H. Nguyen, J. Yamagishi, and I. Echizen, “Use of a Capsule Network to Detect Fake Images and Videos,” arXiv preprint arXiv:1910.12467. 2019 Oct 29."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FxK-zulhAabC",
        "colab_type": "text"
      },
      "source": [
        "## 1. Import dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-XzVj-mmYxo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "import torch.backends.cudnn as cudnn\n",
        "import numpy as np\n",
        "from torch.autograd import Variable\n",
        "from torch.optim import Adam\n",
        "import torchvision.models as models\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "from tqdm import tqdm\n",
        "from sklearn import metrics"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_b5xxMz-Siy",
        "colab_type": "text"
      },
      "source": [
        "## 2. Capsule Network architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8I8i66agmSV9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NO_CAPS = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2EVEaLvmXIU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class StatsNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(StatsNet, self).__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.data.shape[0], x.data.shape[1], x.data.shape[2]*x.data.shape[3])\n",
        "\n",
        "        mean = torch.mean(x, 2)\n",
        "        std = torch.std(x, 2)\n",
        "\n",
        "        return torch.stack((mean, std), dim=1)\n",
        "\n",
        "class View(nn.Module):\n",
        "    def __init__(self, *shape):\n",
        "        super(View, self).__init__()\n",
        "        self.shape = shape\n",
        "\n",
        "    def forward(self, input):\n",
        "        return input.view(self.shape)\n",
        "\n",
        "class VggExtractor(nn.Module):\n",
        "    def __init__(self, train=False):\n",
        "        super(VggExtractor, self).__init__()\n",
        "\n",
        "        self.vgg_1 = self.Vgg(models.vgg19(pretrained=True), 0, 18)\n",
        "        if train:\n",
        "            self.vgg_1.train(mode=True)\n",
        "            self.freeze_gradient()\n",
        "        else:\n",
        "            self.vgg_1.eval()\n",
        "\n",
        "    def Vgg(self, vgg, begin, end):\n",
        "        features = nn.Sequential(*list(vgg.features.children())[begin:(end+1)])\n",
        "        return features\n",
        "\n",
        "    def freeze_gradient(self, begin=0, end=9):\n",
        "        for i in range(begin, end+1):\n",
        "            self.vgg_1[i].requires_grad = False\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.vgg_1(input)\n",
        "\n",
        "class FeatureExtractor(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FeatureExtractor, self).__init__()\n",
        "\n",
        "        self.capsules = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Conv2d(256, 64, kernel_size=3, stride=1, padding=1),\n",
        "                nn.BatchNorm2d(64),\n",
        "                nn.ReLU(),\n",
        "                nn.Conv2d(64, 16, kernel_size=3, stride=1, padding=1),\n",
        "                nn.BatchNorm2d(16),\n",
        "                nn.ReLU(),\n",
        "                StatsNet(),\n",
        "\n",
        "                nn.Conv1d(2, 8, kernel_size=5, stride=2, padding=2),\n",
        "                nn.BatchNorm1d(8),\n",
        "                nn.Conv1d(8, 1, kernel_size=3, stride=1, padding=1),\n",
        "                nn.BatchNorm1d(1),\n",
        "                View(-1, 8),\n",
        "                )\n",
        "                for _ in range(NO_CAPS)]\n",
        "        )\n",
        "\n",
        "    def squash(self, tensor, dim):\n",
        "        squared_norm = (tensor ** 2).sum(dim=dim, keepdim=True)\n",
        "        scale = squared_norm / (1 + squared_norm)\n",
        "        return scale * tensor / (torch.sqrt(squared_norm))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # outputs = [capsule(x.detach()) for capsule in self.capsules]\n",
        "        # outputs = [capsule(x.clone()) for capsule in self.capsules]\n",
        "        outputs = [capsule(x) for capsule in self.capsules]\n",
        "        output = torch.stack(outputs, dim=-1)\n",
        "\n",
        "        return self.squash(output, dim=-1)\n",
        "\n",
        "class RoutingLayer(nn.Module):\n",
        "    def __init__(self, gpu_id, num_input_capsules, num_output_capsules, data_in, data_out, num_iterations):\n",
        "        super(RoutingLayer, self).__init__()\n",
        "\n",
        "        self.gpu_id = gpu_id\n",
        "        self.num_iterations = num_iterations\n",
        "        self.route_weights = nn.Parameter(torch.randn(num_output_capsules, num_input_capsules, data_out, data_in))\n",
        "\n",
        "\n",
        "    def squash(self, tensor, dim):\n",
        "        squared_norm = (tensor ** 2).sum(dim=dim, keepdim=True)\n",
        "        scale = squared_norm / (1 + squared_norm)\n",
        "        return scale * tensor / (torch.sqrt(squared_norm))\n",
        "\n",
        "    def forward(self, x, random, dropout):\n",
        "        # x[b, data, in_caps]\n",
        "\n",
        "        x = x.transpose(2, 1)\n",
        "        # x[b, in_caps, data]\n",
        "\n",
        "        if random:\n",
        "            noise = Variable(0.01*torch.randn(*self.route_weights.size()))\n",
        "            if self.gpu_id >= 0:\n",
        "                noise = noise.cuda(self.gpu_id)\n",
        "            route_weights = self.route_weights + noise\n",
        "        else:\n",
        "            route_weights = self.route_weights\n",
        "\n",
        "        priors = route_weights[:, None, :, :, :] @ x[None, :, :, :, None]\n",
        "\n",
        "        # route_weights [out_caps , 1 , in_caps , data_out , data_in]\n",
        "        # x             [   1     , b , in_caps , data_in ,    1    ]\n",
        "        # priors        [out_caps , b , in_caps , data_out,    1    ]\n",
        "\n",
        "        priors = priors.transpose(1, 0)\n",
        "        # priors[b, out_caps, in_caps, data_out, 1]\n",
        "\n",
        "        if dropout > 0.0:\n",
        "            drop = Variable(torch.FloatTensor(*priors.size()).bernoulli(1.0- dropout))\n",
        "            if self.gpu_id >= 0:\n",
        "                drop = drop.cuda(self.gpu_id)\n",
        "            priors = priors * drop\n",
        "            \n",
        "\n",
        "        logits = Variable(torch.zeros(*priors.size()))\n",
        "        # logits[b, out_caps, in_caps, data_out, 1]\n",
        "\n",
        "        if self.gpu_id >= 0:\n",
        "            logits = logits.cuda(self.gpu_id)\n",
        "\n",
        "        num_iterations = self.num_iterations\n",
        "\n",
        "        for i in range(num_iterations):\n",
        "            probs = F.softmax(logits, dim=2)\n",
        "            outputs = self.squash((probs * priors).sum(dim=2, keepdim=True), dim=3)\n",
        "\n",
        "            if i != self.num_iterations - 1:\n",
        "                delta_logits = priors * outputs\n",
        "                logits = logits + delta_logits\n",
        "\n",
        "        # outputs[b, out_caps, 1, data_out, 1]\n",
        "        outputs = outputs.squeeze()\n",
        "\n",
        "        if len(outputs.shape) == 3:\n",
        "            outputs = outputs.transpose(2, 1).contiguous() \n",
        "        else:\n",
        "            outputs = outputs.unsqueeze_(dim=0).transpose(2, 1).contiguous()\n",
        "        # outputs[b, data_out, out_caps]\n",
        "\n",
        "        return outputs\n",
        "\n",
        "class CapsuleNet(nn.Module):\n",
        "    def __init__(self, num_class, gpu_id):\n",
        "        super(CapsuleNet, self).__init__()\n",
        "\n",
        "        self.num_class = num_class\n",
        "        self.fea_ext = FeatureExtractor()\n",
        "        self.fea_ext.apply(self.weights_init)\n",
        "\n",
        "        self.routing_stats = RoutingLayer(gpu_id=gpu_id, num_input_capsules=NO_CAPS, num_output_capsules=num_class, data_in=8, data_out=4, num_iterations=2)\n",
        "\n",
        "    def weights_init(self, m):\n",
        "        classname = m.__class__.__name__\n",
        "        if classname.find('Conv') != -1:\n",
        "            m.weight.data.normal_(0.0, 0.02)\n",
        "        elif classname.find('BatchNorm') != -1:\n",
        "            m.weight.data.normal_(1.0, 0.02)\n",
        "            m.bias.data.fill_(0)\n",
        "\n",
        "    def forward(self, x, random=False, dropout=0.0):\n",
        "\n",
        "        z = self.fea_ext(x)\n",
        "        z = self.routing_stats(z, random, dropout=dropout)\n",
        "        # z[b, data, out_caps]\n",
        "\n",
        "        # classes = F.softmax(z, dim=-1)\n",
        "\n",
        "        # class_ = classes.detach()\n",
        "        # class_ = class_.mean(dim=1)\n",
        "\n",
        "        # return classes, class_\n",
        "\n",
        "        classes = F.softmax(z, dim=-1)\n",
        "        class_ = classes.detach()\n",
        "        class_ = class_.mean(dim=1)\n",
        "\n",
        "        return z, class_\n",
        "\n",
        "class CapsuleLoss(nn.Module):\n",
        "    def __init__(self, gpu_id):\n",
        "        super(CapsuleLoss, self).__init__()\n",
        "        self.cross_entropy_loss = nn.CrossEntropyLoss()\n",
        "\n",
        "        if gpu_id >= 0:\n",
        "            self.cross_entropy_loss.cuda(gpu_id)\n",
        "\n",
        "    def forward(self, classes, labels):\n",
        "        loss_t = self.cross_entropy_loss(classes[:,0,:], labels)\n",
        "\n",
        "        for i in range(classes.size(1) - 1):\n",
        "            loss_t = loss_t + self.cross_entropy_loss(classes[:,i+1,:], labels)\n",
        "\n",
        "        return loss_t"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUKuV589-0Dz",
        "colab_type": "text"
      },
      "source": [
        "## 3. Default settings\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kAyIs5gJoTLH",
        "colab_type": "code",
        "outputId": "ab3ad061-f3fe-429a-b55d-860381ef2f92",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "dataset = os.getcwd() + '/celeb-df'\n",
        "train_set = '/train'\n",
        "val_set = '/validation'\n",
        "test_set = '/test'\n",
        "workers = 0\n",
        "batch_size = 32\n",
        "image_size = 300\n",
        "learning_rate = 0.0005\n",
        "beta1_for_adam = 0.9\n",
        "gpu_id = 0\n",
        "manual_seed = random.randint(1, 1000)\n",
        "print(\"Random Seed: \", manual_seed)\n",
        "random.seed(manual_seed)\n",
        "torch.manual_seed(manual_seed)\n",
        "torch.cuda.manual_seed_all(manual_seed)\n",
        "cudnn.benchmark = True\n",
        "disable_random_routing_matrix = False\n",
        "random_setting = not disable_random_routing_matrix\n",
        "text_writer_train = open(os.path.join(os.getcwd(), 'train.csv'), 'a')\n",
        "text_writer_test = open(os.path.join(os.getcwd(), 'test.txt'), 'w')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Random Seed:  287\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QB1JiORc-_J",
        "colab_type": "text"
      },
      "source": [
        "## 4. Creating Capsule"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Omwg2p0bnqmC",
        "colab_type": "code",
        "outputId": "4b39f943-998a-4092-962a-076a711b0227",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136,
          "referenced_widgets": [
            "3a5bb1c2e10847cbb04e6a703bf17c28",
            "0ba4c5f24a0c4b97ae96229228a0daea",
            "0c10db0e76a34c9c810550cc2ce80a4d",
            "210d93ccd52c46a28e258fb04be39c19",
            "8db5187646864526bdaa281297a98236",
            "daad4ffa3edc4f168c47f07c01af2138",
            "3c3d467680a24822bbea50937d98c903",
            "13a626f1f9f747b5b727982b921d223c"
          ]
        }
      },
      "source": [
        "vgg_ext = VggExtractor()\n",
        "capnet = CapsuleNet(2, gpu_id)\n",
        "capsule_loss = CapsuleLoss(gpu_id)\n",
        "optimizer = Adam(capnet.parameters(), lr=learning_rate, betas=(beta1_for_adam, 0.999))\n",
        "\n",
        "capnet.cuda(gpu_id)\n",
        "vgg_ext.cuda(gpu_id)\n",
        "capsule_loss.cuda(gpu_id)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/checkpoints/vgg19-dcbb9e9d.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3a5bb1c2e10847cbb04e6a703bf17c28",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, max=574673361), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CapsuleLoss(\n",
              "  (cross_entropy_loss): CrossEntropyLoss()\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6agcWVSsdYA6",
        "colab_type": "text"
      },
      "source": [
        "## 5. Creating datasets for training, validation, and test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8PkE7_INQx7",
        "colab_type": "code",
        "outputId": "730a6468-fe4d-446e-b4fb-69a8fcce1289",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        }
      },
      "source": [
        "transform_fwd = transforms.Compose([\n",
        "        transforms.Resize((image_size, image_size)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
        "        ])\n",
        "\n",
        "dataset_train = dset.ImageFolder(root=dataset + train_set, transform=transform_fwd)\n",
        "assert dataset_train\n",
        "dataloader_train = torch.utils.data.DataLoader(dataset_train, batch_size=batch_size, shuffle=True, num_workers=int(workers))\n",
        "\n",
        "dataset_val = dset.ImageFolder(root=dataset + val_set, transform=transform_fwd)\n",
        "assert dataset_val\n",
        "dataloader_val = torch.utils.data.DataLoader(dataset_val, batch_size=batch_size, shuffle=False, num_workers=int(workers))\n",
        "\n",
        "dataset_test = dset.ImageFolder(root=dataset + test_set, transform=transform_fwd)\n",
        "assert dataset_test\n",
        "dataloader_test = torch.utils.data.DataLoader(dataset_test, batch_size=batch_size, shuffle=False, num_workers=int(workers))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-d2598c804728>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m         ])\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdataset_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform_fwd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mdataset_train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mdataloader_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file)\u001b[0m\n\u001b[1;32m    207\u001b[0m                                           \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m                                           \u001b[0mtarget_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m                                           is_valid_file=is_valid_file)\n\u001b[0m\u001b[1;32m    210\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m             raise (RuntimeError(\"Found 0 files in subfolders of: \" + self.root + \"\\n\"\n\u001b[0;32m---> 97\u001b[0;31m                                 \"Supported extensions are: \" + \",\".join(extensions)))\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Found 0 files in subfolders of: /content/celeb-df/train\nSupported extensions are: .jpg,.jpeg,.png,.ppm,.bmp,.pgm,.tif,.tiff,.webp"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FS1vY39jddg-",
        "colab_type": "text"
      },
      "source": [
        "## 6. Start training phase\n",
        "- Save the fitted model at the end"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGWdzEJ4QKLJ",
        "colab_type": "code",
        "outputId": "209ce60a-fa08-4229-9e9d-d7279af08772",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for epoch in range(1, 26):\n",
        "        count = 0\n",
        "        loss_train = 0\n",
        "        loss_test = 0\n",
        "\n",
        "        tol_label = np.array([], dtype=np.float)\n",
        "        tol_pred = np.array([], dtype=np.float)\n",
        "\n",
        "        for img_data, labels_data in tqdm(dataloader_train):\n",
        "\n",
        "            labels_data[labels_data > 1] = 1\n",
        "            img_label = labels_data.numpy().astype(np.float)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            if gpu_id >= 0:\n",
        "              img_data = img_data.cuda(gpu_id)\n",
        "              labels_data = labels_data.cuda(gpu_id)\n",
        "\n",
        "            input_v = Variable(img_data)\n",
        "            x = vgg_ext(input_v)\n",
        "            classes, class_ = capnet(x, random=optrandom, dropout=0.05)\n",
        "\n",
        "            loss_dis = capsule_loss(classes, Variable(labels_data, requires_grad=False))\n",
        "            loss_dis_data = loss_dis.item()\n",
        "\n",
        "            loss_dis.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            output_dis = class_.data.cpu().numpy()\n",
        "            output_pred = np.zeros((output_dis.shape[0]), dtype=np.float)\n",
        "\n",
        "            for i in range(output_dis.shape[0]):\n",
        "                if output_dis[i,1] >= output_dis[i,0]:\n",
        "                    output_pred[i] = 1.0\n",
        "                else:\n",
        "                    output_pred[i] = 0.0\n",
        "\n",
        "            tol_label = np.concatenate((tol_label, img_label))\n",
        "            tol_pred = np.concatenate((tol_pred, output_pred))\n",
        "\n",
        "            loss_train += loss_dis_data\n",
        "            count += 1\n",
        "\n",
        "\n",
        "        acc_train = metrics.accuracy_score(tol_label, tol_pred)\n",
        "        loss_train /= count\n",
        "\n",
        "        ########################################################################\n",
        "\n",
        "        # do checkpointing & validation\n",
        "        torch.save(capnet.state_dict(), os.path.join(os.getcwd(), 'capsule_%d.pt' % epoch))\n",
        "        torch.save(optimizer.state_dict(), os.path.join(os.getcwd(), 'optim_%d.pt' % epoch))\n",
        "\n",
        "        capnet.eval()\n",
        "\n",
        "        tol_label = np.array([], dtype=np.float)\n",
        "        tol_pred = np.array([], dtype=np.float)\n",
        "\n",
        "        count = 0\n",
        "\n",
        "        for img_data, labels_data in dataloader_val:\n",
        "\n",
        "            labels_data[labels_data > 1] = 1\n",
        "            img_label = labels_data.numpy().astype(np.float)\n",
        "\n",
        "            if gpu_id >= 0:\n",
        "              img_data = img_data.cuda(gpu_id)\n",
        "              labels_data = labels_data.cuda(gpu_id)\n",
        "\n",
        "            input_v = Variable(img_data)\n",
        "\n",
        "            x = vgg_ext(input_v)\n",
        "            classes, class_ = capnet(x, random=False)\n",
        "\n",
        "            loss_dis = capsule_loss(classes, Variable(labels_data, requires_grad=False))\n",
        "            loss_dis_data = loss_dis.item()\n",
        "            output_dis = class_.data.cpu().numpy()\n",
        "\n",
        "            output_pred = np.zeros((output_dis.shape[0]), dtype=np.float)\n",
        "\n",
        "            for i in range(output_dis.shape[0]):\n",
        "                if output_dis[i,1] >= output_dis[i,0]:\n",
        "                    output_pred[i] = 1.0\n",
        "                else:\n",
        "                    output_pred[i] = 0.0\n",
        "\n",
        "            tol_label = np.concatenate((tol_label, img_label))\n",
        "            tol_pred = np.concatenate((tol_pred, output_pred))\n",
        "\n",
        "            loss_test += loss_dis_data\n",
        "            count += 1\n",
        "\n",
        "        acc_test = metrics.accuracy_score(tol_label, tol_pred)\n",
        "        loss_test /= count\n",
        "\n",
        "        print('[Epoch %d] Train loss: %.4f   acc: %.2f | Test loss: %.4f  acc: %.2f'\n",
        "        % (epoch, loss_train, acc_train*100, loss_test, acc_test*100))\n",
        "\n",
        "        text_writer.write('%d,%.4f,%.2f,%.4f,%.2f\\n'\n",
        "        % (epoch, loss_train, acc_train*100, loss_test, acc_test*100))\n",
        "\n",
        "        text_writer.flush()\n",
        "        capnet.train(mode=True)\n",
        "\n",
        "text_writer.close()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  1.34it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch 1] Train loss: 2.7654   acc: 100.00 | Test loss: 2.7647  acc: 100.00\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  1.47it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch 2] Train loss: 1.6929   acc: 100.00 | Test loss: 2.7656  acc: 100.00\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  1.46it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch 3] Train loss: 1.6828   acc: 100.00 | Test loss: 2.7663  acc: 100.00\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  1.48it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch 4] Train loss: 1.6399   acc: 100.00 | Test loss: 2.7670  acc: 100.00\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  1.46it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch 5] Train loss: 1.6405   acc: 100.00 | Test loss: 2.7675  acc: 100.00\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  1.47it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch 6] Train loss: 1.6311   acc: 100.00 | Test loss: 2.7680  acc: 100.00\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  1.49it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch 7] Train loss: 1.6214   acc: 100.00 | Test loss: 2.7684  acc: 100.00\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  1.48it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch 8] Train loss: 1.6075   acc: 100.00 | Test loss: 2.7687  acc: 100.00\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  1.46it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch 9] Train loss: 1.5987   acc: 100.00 | Test loss: 2.7690  acc: 100.00\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  1.45it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch 10] Train loss: 1.5939   acc: 100.00 | Test loss: 2.7692  acc: 100.00\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  1.47it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch 11] Train loss: 1.5998   acc: 100.00 | Test loss: 2.7694  acc: 100.00\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  1.45it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch 12] Train loss: 1.5997   acc: 100.00 | Test loss: 2.7696  acc: 100.00\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  1.45it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch 13] Train loss: 1.5835   acc: 100.00 | Test loss: 2.7698  acc: 100.00\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  1.47it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch 14] Train loss: 1.5941   acc: 100.00 | Test loss: 2.7699  acc: 100.00\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  1.41it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch 15] Train loss: 1.5962   acc: 100.00 | Test loss: 2.7700  acc: 100.00\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  1.43it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch 16] Train loss: 1.5974   acc: 100.00 | Test loss: 2.7701  acc: 100.00\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  1.47it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch 17] Train loss: 1.6057   acc: 100.00 | Test loss: 2.7701  acc: 100.00\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  1.46it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch 18] Train loss: 1.5824   acc: 100.00 | Test loss: 2.7701  acc: 100.00\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  1.46it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch 19] Train loss: 1.5678   acc: 100.00 | Test loss: 2.7701  acc: 100.00\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  1.41it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch 20] Train loss: 1.5796   acc: 100.00 | Test loss: 2.7701  acc: 100.00\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  1.47it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch 21] Train loss: 1.5892   acc: 100.00 | Test loss: 2.7700  acc: 100.00\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  1.43it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch 22] Train loss: 1.5883   acc: 100.00 | Test loss: 2.7699  acc: 100.00\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  1.42it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch 23] Train loss: 1.5779   acc: 100.00 | Test loss: 2.7698  acc: 100.00\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  1.44it/s]\n",
            "  0%|          | 0/1 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch 24] Train loss: 1.5814   acc: 100.00 | Test loss: 2.7696  acc: 100.00\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  1.40it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[Epoch 25] Train loss: 1.5883   acc: 100.00 | Test loss: 2.7694  acc: 100.00\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytz64QU1EzcE",
        "colab_type": "text"
      },
      "source": [
        "## 7. Start evaluation phase"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVjZ1JLfi-m2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "capnet.load_state_dict(torch.load(os.path.join(os.getcwd(),'capsule_' + str(21) + '.pt')))\n",
        "capnet.eval()\n",
        "\n",
        "tol_label = np.array([], dtype=np.float)\n",
        "tol_pred = np.array([], dtype=np.float)\n",
        "tol_pred_prob = np.array([], dtype=np.float)\n",
        "\n",
        "count = 0\n",
        "loss_test = 0\n",
        "\n",
        "for img_data, labels_data in tqdm(dataloader_test):\n",
        "\n",
        "        labels_data[labels_data > 1] = 1\n",
        "        img_label = labels_data.numpy().astype(np.float)\n",
        "\n",
        "        if gpu_id >= 0:\n",
        "          img_data = img_data.cuda(gpu_id)\n",
        "          labels_data = labels_data.cuda(gpu_id)\n",
        "\n",
        "        input_v = Variable(img_data)\n",
        "\n",
        "        x = vgg_ext(input_v)\n",
        "        classes, class_ = capnet(x, random=random_setting)\n",
        "\n",
        "        output_dis = class_.data.cpu()\n",
        "        output_pred = np.zeros((output_dis.shape[0]), dtype=np.float)\n",
        "\n",
        "        for i in range(output_dis.shape[0]):\n",
        "            if output_dis[i,1] >= output_dis[i,0]:\n",
        "                output_pred[i] = 1.0\n",
        "            else:\n",
        "                output_pred[i] = 0.0\n",
        "\n",
        "        tol_label = np.concatenate((tol_label, img_label))\n",
        "        tol_pred = np.concatenate((tol_pred, output_pred))\n",
        "        \n",
        "        pred_prob = torch.softmax(output_dis, dim=1)\n",
        "        tol_pred_prob = np.concatenate((tol_pred_prob, pred_prob[:,1].data.numpy()))\n",
        "\n",
        "        count += 1\n",
        "\n",
        "acc_test = metrics.accuracy_score(tol_label, tol_pred)\n",
        "loss_test /= count\n",
        "\n",
        "fpr, tpr, thresholds = roc_curve(tol_label, tol_pred_prob, pos_label=1)\n",
        "eer = brentq(lambda x : 1. - x - interp1d(fpr, tpr)(x), 0., 1.)\n",
        "\n",
        "# fnr = 1 - tpr\n",
        "# hter = (fpr + fnr)/2\n",
        "\n",
        "print('[Epoch %d] Test acc: %.2f   EER: %.2f' % (21, acc_test*100, eer*100))\n",
        "text_writer_test.write('%d,%.2f,%.2f\\n'% (21, acc_test*100, eer*100))\n",
        "\n",
        "text_writer_test.flush()\n",
        "text_writer_test.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mj8hxuMgWhpe",
        "colab_type": "text"
      },
      "source": [
        "# Train ClassNSeg model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fvKFyADBmwvM",
        "colab_type": "text"
      },
      "source": [
        "**Public repository:** \n",
        "https://github.com/nii-yamagishilab/ClassNSeg \n",
        "\n",
        "**Reference:**\n",
        "H. H. Nguyen, F. Fang, J. Yamagishi, and I. Echizen, “Multi-task Learning for Detecting and Segmenting Manipulated Facial Images and Videos,” Proc. of the 10th IEEE International Conference on Biometrics: Theory, Applications and Systems (BTAS), 8 pages, (September 2019)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vewK3OwGoESG",
        "colab_type": "text"
      },
      "source": [
        "## 1. Import dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHKeQVSwWmTX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.backends.cudnn as cudnn\n",
        "from torch import nn\n",
        "from torch.optim import Adam\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "from tqdm import tqdm\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "import random"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1ieNfPNoGGp",
        "colab_type": "text"
      },
      "source": [
        "## 2. ClassNSeg Network architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdQHFvXEWrBE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, depth=3):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(depth, 8, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(8),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.Conv2d(8, 8, kernel_size=3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(8),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.Conv2d(8, 16, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.Conv2d(16, 16, kernel_size=3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.Conv2d(32, 32, kernel_size=3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        self.encoder.apply(self.weights_init)\n",
        "\n",
        "    def weights_init(self, m):\n",
        "        classname = m.__class__.__name__\n",
        "        if classname.find('Conv') != -1:\n",
        "            m.weight.data.normal_(0.0, 0.02)\n",
        "        elif classname.find('BatchNorm') != -1:\n",
        "            m.weight.data.normal_(0.5, 0.02)\n",
        "            m.bias.data.fill_(0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.encoder(x)\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, depth=3):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.shared = nn.Sequential(\n",
        "            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=1, padding=1, output_padding=0),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.ConvTranspose2d(64, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=1, padding=1, output_padding=0),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.ConvTranspose2d(32, 32, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        self.segmenter = nn.Sequential(\n",
        "\n",
        "            nn.ConvTranspose2d(32, 16, kernel_size=3, stride=1, padding=1, output_padding=0),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.ConvTranspose2d(16, 16, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.ConvTranspose2d(16, 8, kernel_size=3, stride=1, padding=1, output_padding=0),\n",
        "            nn.BatchNorm2d(8),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.ConvTranspose2d(8, 8, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
        "            nn.BatchNorm2d(8),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.ConvTranspose2d(8, 2, kernel_size=3, stride=1, padding=1, output_padding=0),\n",
        "            nn.Softmax(dim=1)\n",
        "        )\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "\n",
        "            nn.ConvTranspose2d(32, 16, kernel_size=3, stride=1, padding=1, output_padding=0),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.ConvTranspose2d(16, 16, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.ConvTranspose2d(16, 8, kernel_size=3, stride=1, padding=1, output_padding=0),\n",
        "            nn.BatchNorm2d(8),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.ConvTranspose2d(8, 8, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
        "            nn.BatchNorm2d(8),\n",
        "            nn.ReLU(),\n",
        "\n",
        "            nn.ConvTranspose2d(8, depth, kernel_size=3, stride=1, padding=1, output_padding=0),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "        self.segmenter.apply(self.weights_init)\n",
        "        self.decoder.apply(self.weights_init)\n",
        "\n",
        "    def weights_init(self, m):\n",
        "        classname = m.__class__.__name__\n",
        "        if classname.find('Conv') != -1:\n",
        "            m.weight.data.normal_(0.0, 0.02)\n",
        "        elif classname.find('BatchNorm') != -1:\n",
        "            m.weight.data.normal_(0.5, 0.02)\n",
        "            m.bias.data.fill_(0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        latent = self.shared(x)\n",
        "        seg = self.segmenter(latent)\n",
        "        rect = self.decoder(latent)\n",
        "\n",
        "        return seg, rect\n",
        "\n",
        "class ActivationLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ActivationLoss, self).__init__()\n",
        "\n",
        "    def forward(self, zero, one, labels):\n",
        "\n",
        "        loss_act = torch.abs(one - labels.data) + torch.abs(zero - (1.0 - labels.data))\n",
        "        return 1 / labels.shape[0] * loss_act.sum()\n",
        "        \n",
        "class ReconstructionLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ReconstructionLoss, self).__init__()\n",
        "        self.loss = nn.MSELoss()\n",
        "\n",
        "    def forward(self, reconstruction, groundtruth):\n",
        "\n",
        "        return self.loss(reconstruction, groundtruth.data)\n",
        "\n",
        "class SegmentationLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SegmentationLoss, self).__init__()\n",
        "        self.loss = nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, segment, groundtruth):\n",
        "\n",
        "        return self.loss(segment.view(segment.shape[0], segment.shape[1], segment.shape[2] * segment.shape[3]), \n",
        "            groundtruth.data.view(groundtruth.shape[0], groundtruth.shape[1] * groundtruth.shape[2]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "15sywLwhoVVb",
        "colab_type": "text"
      },
      "source": [
        "## 3. Default settings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bEQ2FN3gE4fI",
        "colab_type": "code",
        "outputId": "b80c9ee5-026a-4f18-bfa6-3b34a3c9496e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "dataset = '/celeb-df'\n",
        "train_set = '/train'\n",
        "val_set = '/validation'\n",
        "test_set = '/test'\n",
        "workers = 0\n",
        "batch_size = 64\n",
        "manual_seed = random.randint(1, 1000)\n",
        "print(\"Random Seed: \", manual_seed)\n",
        "random.seed(manual_seed)\n",
        "torch.manual_seed(manual_seed)\n",
        "torch.cuda.manual_seed_all(manual_seed)\n",
        "cudnn.benchmark = True\n",
        "learning_rate = 0.001\n",
        "beta1_for_adam = 0.9\n",
        "weight_decay = 0.0005\n",
        "eps = 1e-07\n",
        "gpu_id = 0\n",
        "gamma = 1\n",
        "text_writer_train = open(os.path.join(os.getcwd(), 'train.csv'), 'a')\n",
        "text_writer_test = open(os.path.join(os.getcwd(), 'classification.txt'), 'w')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Random Seed:  371\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3JlxE0xobTQ",
        "colab_type": "text"
      },
      "source": [
        "## 4. Creating ClassNSeg"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rkxrZ24codA1",
        "colab_type": "code",
        "outputId": "482cd9c2-0a9e-4350-b12c-c47bc4b54cec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "encoder = Encoder(3)\n",
        "decoder = Decoder(3)\n",
        "act_loss_fn = ActivationLoss()\n",
        "rect_loss_fn = ReconstructionLoss()\n",
        "seg_loss_fn = SegmentationLoss()\n",
        "\n",
        "optimizer_encoder = Adam(encoder.parameters(), lr=learning_rate, betas=(beta1_for_adam, 0.999), weight_decay=weight_decay, eps=eps)\n",
        "optimizer_decoder = Adam(decoder.parameters(), lr=learning_rate, betas=(beta1_for_adam, 0.999), weight_decay=weight_decay, eps=eps)\n",
        "\n",
        "encoder.cuda(gpu_id)\n",
        "decoder.cuda(gpu_id)\n",
        "act_loss_fn.cuda(gpu_id)\n",
        "seg_loss_fn.cuda(gpu_id)\n",
        "rect_loss_fn.cuda(gpu_id)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ReconstructionLoss(\n",
              "  (loss): MSELoss()\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTZxQ_9cE0Yf",
        "colab_type": "text"
      },
      "source": [
        "## 5. Start pre-processing phase"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUriqyjVXLnI",
        "colab_type": "code",
        "outputId": "6bf665d4-f264-497d-8f7d-6cba177fb44a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        }
      },
      "source": [
        "input_real = os.getcwd() + '/celeb-df/validation/0_real_videos/'\n",
        "input_fake = os.getcwd() + '/celeb-df/validation/1_fake_videos/'\n",
        "mask = ''\n",
        "output_real = os.getcwd() + '/deepfakes/real'\n",
        "output_fake = os.getcwd() + '/deepfakes/fake'\n",
        "image_size = 256\n",
        "limit = 10\n",
        "scale = 1.3\n",
        "\n",
        "def to_bw(mask, thresh_binary=10, thresh_otsu=255):\n",
        "    im_gray = cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY)\n",
        "    (thresh, im_bw) = cv2.threshold(im_gray, thresh_binary, thresh_otsu, cv2.THRESH_BINARY | cv2.THRESH_OTSU)\n",
        "\n",
        "    return im_bw\n",
        "\n",
        "def get_bbox(mask, thresh_binary=127, thresh_otsu=255):\n",
        "    im_bw = to_bw(mask, thresh_binary, thresh_otsu)\n",
        "\n",
        "    # im2, contours, hierarchy = cv2.findContours(im_bw,cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)\n",
        "    contours, hierarchy = cv2.findContours(im_bw,cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "    locations = np.array([], dtype=np.int).reshape(0, 5)\n",
        "\n",
        "    for c in contours:\n",
        "        # calculate moments for each contour\n",
        "        M = cv2.moments(c)\n",
        "        if M[\"m00\"] > 0:\n",
        "            cX = int(M[\"m10\"] / M[\"m00\"])\n",
        "        else:\n",
        "            cX = 0\n",
        "        if M[\"m00\"] > 0:\n",
        "            cY = int(M[\"m01\"] / M[\"m00\"])\n",
        "        else:\n",
        "            cY = 0\n",
        "\n",
        "        # calculate the rectangle bounding box\n",
        "        x,y,w,h = cv2.boundingRect(c)\n",
        "        locations = np.concatenate((locations, np.array([[cX, cY, w, h, w + h]])), axis=0)\n",
        "\n",
        "    max_idex = locations[:,4].argmax()\n",
        "    bbox = locations[max_idex, 0:4].reshape(4)\n",
        "    return bbox\n",
        "\n",
        "def extract_face(image, bbox, scale = 1.0):\n",
        "    h, w, d = image.shape\n",
        "    radius = int(bbox[3] * scale / 2)\n",
        "\n",
        "    y_1 = bbox[1] - radius\n",
        "    y_2 = bbox[1] + radius\n",
        "    x_1 = bbox[0] - radius\n",
        "    x_2 = bbox[0] + radius\n",
        "\n",
        "    if x_1 < 0:\n",
        "        x_1 = 0\n",
        "    if y_1 < 0:\n",
        "        y_1 = 0\n",
        "    if x_2 > w:\n",
        "        x_2 = w\n",
        "    if y_2 > h:\n",
        "        y_2 = h\n",
        "\n",
        "    crop_img = image[y_1:y_2, x_1:x_2]\n",
        "\n",
        "    if crop_img is not None:\n",
        "        crop_img = cv2.resize(crop_img, (image_size, image_size))\n",
        "\n",
        "    return crop_img\n",
        "\n",
        "def extract_face_videos(input_real, input_fake, input_mask, output_real, output_fake):\n",
        "\n",
        "    blank_img = np.zeros((image_size,image_size,3), np.uint8)\n",
        "\n",
        "    for f in os.listdir(input_fake):\n",
        "        if os.path.isfile(os.path.join(input_fake, f)):\n",
        "            if f.lower().endswith(('mp4')):\n",
        "                print(f)\n",
        "                filename = os.path.splitext(f)[0]\n",
        "\n",
        "                vidcap_fake = cv2.VideoCapture(os.path.join(input_fake, f))\n",
        "                success_fake, image_fake = vidcap_fake.read()\n",
        "                print(success_fake)\n",
        "\n",
        "                image_mask = cv2.imread(os.path.join(input_mask, filename, '0000.png'))\n",
        "\n",
        "                count = 0\n",
        "\n",
        "                while (success_fake):\n",
        "\n",
        "                    bbox = get_bbox(image_mask)\n",
        "\n",
        "                    altered_cropped = extract_face(image_fake, bbox, scale)\n",
        "\n",
        "                    mask_cropped = to_bw(extract_face(image_mask, bbox, scale))\n",
        "                    mask_cropped = np.stack((mask_cropped,mask_cropped, mask_cropped), axis=2)\n",
        "\n",
        "                    altered_cropped = np.concatenate((altered_cropped, mask_cropped), axis=1)\n",
        "\n",
        "                    cv2.imwrite(os.path.join(output_fake, filename + \"_%d.jpg\" % count), altered_cropped)\n",
        "\n",
        "                    count += 1\n",
        "\n",
        "                    if count >= limit:\n",
        "                        break\n",
        "\n",
        "                    print(\"still\")\n",
        "\n",
        "                    success_fake, image_fake = vidcap_fake.read()\n",
        "                    image_mask = cv2.imread(os.path.join(input_mask, filename, str(count).zfill(4) + '.png'))\n",
        "\n",
        "extract_face_videos(input_real, input_fake, mask, output_real, output_fake)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "id0_id16_0005.mp4\n",
            "True\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "error",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-4b44508ffb81>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    108\u001b[0m                     \u001b[0mimage_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m \u001b[0mextract_face_videos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_real\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_fake\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_real\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_fake\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-8-4b44508ffb81>\u001b[0m in \u001b[0;36mextract_face_videos\u001b[0;34m(input_real, input_fake, input_mask, output_real, output_fake)\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msuccess_fake\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                     \u001b[0mbbox\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_bbox\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m                     \u001b[0maltered_cropped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_face\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_fake\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-4b44508ffb81>\u001b[0m in \u001b[0;36mget_bbox\u001b[0;34m(mask, thresh_binary, thresh_otsu)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_bbox\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthresh_binary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m127\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthresh_otsu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mim_bw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_bw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthresh_binary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthresh_otsu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# im2, contours, hierarchy = cv2.findContours(im_bw,cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-4b44508ffb81>\u001b[0m in \u001b[0;36mto_bw\u001b[0;34m(mask, thresh_binary, thresh_otsu)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mto_bw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthresh_binary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthresh_otsu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mim_gray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0mthresh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mim_bw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim_gray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthresh_binary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthresh_otsu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTHRESH_BINARY\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTHRESH_OTSU\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31merror\u001b[0m: OpenCV(4.1.2) /io/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGl_QbmMonwA",
        "colab_type": "text"
      },
      "source": [
        "## 6. Creating datasets for training, validation, and test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RImo2nnSoov2",
        "colab_type": "code",
        "outputId": "d00c9688-c9d2-4636-e9f0-6b76d6da0816",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        }
      },
      "source": [
        "class Normalize_3D(object):\n",
        "        def __init__(self, mean, std):\n",
        "            self.mean = mean\n",
        "            self.std = std\n",
        "\n",
        "        def __call__(self, tensor):\n",
        "            \"\"\"\n",
        "                Tensor: Normalized image.\n",
        "            Args:\n",
        "                tensor (Tensor): Tensor image of size (C, H, W) to be normalized.\n",
        "            Returns:        \"\"\"\n",
        "            for t, m, s in zip(tensor, self.mean, self.std):\n",
        "                t.sub_(m).div_(s)\n",
        "            return tensor\n",
        "\n",
        "class UnNormalize_3D(object):\n",
        "        def __init__(self, mean, std):\n",
        "            self.mean = mean\n",
        "            self.std = std\n",
        "\n",
        "        def __call__(self, tensor):\n",
        "            \"\"\"\n",
        "                Tensor: Normalized image.\n",
        "            Args:\n",
        "                tensor (Tensor): Tensor image of size (C, H, W) to be normalized.\n",
        "            Returns:        \"\"\"\n",
        "            for t, m, s in zip(tensor, self.mean, self.std):\n",
        "                t.mul_(s).add_(m)\n",
        "            return tensor\n",
        "\n",
        "transform_tns = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "    ])\n",
        "\n",
        "transform_pil = transforms.Compose([\n",
        "        transforms.ToPILImage(),\n",
        "    ])\n",
        "    \n",
        "transform_norm = Normalize_3D((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
        "\n",
        "transform_unnorm = UnNormalize_3D((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
        "\n",
        "dataset_train = dset.ImageFolder(root=os.path.join(dataset, train_set), transform=transform_tns)\n",
        "assert dataset_train\n",
        "dataloader_train = torch.utils.data.DataLoader(dataset_train, batch_size=batch_size, shuffle=True, num_workers=int(workers))\n",
        "\n",
        "dataset_val = dset.ImageFolder(root=os.path.join(dataset, val_set), transform=transform_tns)\n",
        "assert dataset_val\n",
        "dataloader_val = torch.utils.data.DataLoader(dataset_val, batch_size=batch_size, shuffle=False, num_workers=int(workers))\n",
        "\n",
        "dataset_test = dset.ImageFolder(root=os.path.join(dataset, test_set), transform=transform_tns)\n",
        "assert dataset_test\n",
        "dataloader_test = torch.utils.data.DataLoader(dataset_test, batch_size=batch_size, shuffle=False, num_workers=int(workers))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-2aa4938d98d1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0mtransform_unnorm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUnNormalize_3D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.485\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.456\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.406\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0.229\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.224\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.225\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0mdataset_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform_tns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mdataset_train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0mdataloader_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file)\u001b[0m\n\u001b[1;32m    207\u001b[0m                                           \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m                                           \u001b[0mtarget_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m                                           is_valid_file=is_valid_file)\n\u001b[0m\u001b[1;32m    210\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001b[0m\n\u001b[1;32m     91\u001b[0m         super(DatasetFolder, self).__init__(root, transform=transform,\n\u001b[1;32m     92\u001b[0m                                             target_transform=target_transform)\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_find_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m         \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextensions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_valid_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m_find_classes\u001b[0;34m(self, dir)\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# Faster and available in Python 3.5 and above\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/train'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6fZT8kFvory0",
        "colab_type": "text"
      },
      "source": [
        "## 7. Start training phase\n",
        "- Save the fitted model at the end"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ool-TzrjovJW",
        "colab_type": "code",
        "outputId": "7759aecb-1cf6-4bfb-883e-635e3a47ec09",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254
        }
      },
      "source": [
        "for epoch in range(1, 101):\n",
        "        count = 0\n",
        "        loss_act_train = 0.0\n",
        "        loss_seg_train = 0.0\n",
        "        loss_rect_train = 0.0\n",
        "        loss_act_test = 0.0\n",
        "        loss_seg_test = 0.0\n",
        "        loss_rect_test = 0.0\n",
        "\n",
        "        tol_label = np.array([], dtype=np.float)\n",
        "        tol_pred = np.array([], dtype=np.float)\n",
        "\n",
        "        for fft_data, labels_data in tqdm(dataloader_train):\n",
        "\n",
        "            optimizer_encoder.zero_grad()\n",
        "            optimizer_decoder.zero_grad()\n",
        "\n",
        "            fft_label = labels_data.numpy().astype(np.float)\n",
        "            labels_data = labels_data.float()\n",
        "\n",
        "            rgb = transform_norm(fft_data[:,:,:,0:256])\n",
        "            mask = fft_data[:,0,:,256:512]\n",
        "            mask[mask >= 0.5] = 1.0\n",
        "            mask[mask < 0.5] = 0.0\n",
        "            mask = mask.long()\n",
        "\n",
        "            if gpu_id >= 0:\n",
        "              rgb = rgb.cuda(gpu_id)\n",
        "              mask = mask.cuda(gpu_id)\n",
        "              labels_data = labels_data.cuda(gpu_id)\n",
        "\n",
        "            latent = encoder(rgb).reshape(-1, 2, 64, 16, 16)\n",
        "\n",
        "            zero_abs = torch.abs(latent[:,0]).view(latent.shape[0], -1)\n",
        "            zero = zero_abs.mean(dim=1)\n",
        "\n",
        "            one_abs = torch.abs(latent[:,1]).view(latent.shape[0], -1)\n",
        "            one = one_abs.mean(dim=1)\n",
        "\n",
        "            loss_act = act_loss_fn(zero, one, labels_data)\n",
        "            loss_act_data = loss_act.item()\n",
        "\n",
        "            y = torch.eye(2)\n",
        "            if gpu_id >= 0:\n",
        "              y = y.cuda(gpu_id)\n",
        "\n",
        "            y = y.index_select(dim=0, index=labels_data.data.long())\n",
        "\n",
        "            latent = (latent * y[:,:,None, None, None]).reshape(-1, 128, 16, 16)\n",
        "\n",
        "            seg, rect = decoder(latent)\n",
        "\n",
        "            loss_seg = seg_loss_fn(seg, mask)\n",
        "            loss_seg = loss_seg * gamma\n",
        "            loss_seg_data = loss_seg.item()\n",
        "\n",
        "            loss_rect = rect_loss_fn(rect, rgb)\n",
        "            loss_rect = loss_rect * gamma\n",
        "            loss_rect_data = loss_rect.item()\n",
        "\n",
        "            loss_total = loss_act + loss_seg + loss_rect\n",
        "            loss_total.backward()\n",
        "\n",
        "            optimizer_decoder.step()\n",
        "            optimizer_encoder.step()\n",
        "\n",
        "            output_pred = np.zeros((fft_data.shape[0]), dtype=np.float)\n",
        "\n",
        "            for i in range(fft_data.shape[0]):\n",
        "                if one[i] >= zero[i]:\n",
        "                    output_pred[i] = 1.0\n",
        "                else:\n",
        "                    output_pred[i] = 0.0\n",
        "\n",
        "            tol_label = np.concatenate((tol_label, fft_label))\n",
        "            tol_pred = np.concatenate((tol_pred, output_pred))\n",
        "\n",
        "            loss_act_train += loss_act_data\n",
        "            loss_seg_train += loss_seg_data\n",
        "            loss_rect_train += loss_rect_data\n",
        "            count += 1\n",
        "\n",
        "        acc_train = metrics.accuracy_score(tol_label, tol_pred)\n",
        "        loss_act_train /= count\n",
        "        loss_seg_train /= count\n",
        "        loss_rect_train /= count\n",
        "\n",
        "        ########################################################################\n",
        "        # do checkpointing & validation\n",
        "\n",
        "        torch.save(encoder.state_dict(), os.path.join(os.getcwd(), 'encoder_%d.pt' % epoch))\n",
        "        torch.save(optimizer_encoder.state_dict(), os.path.join(os.getcwd(), 'optim_encoder_%d.pt' % epoch))\n",
        "\n",
        "        torch.save(decoder.state_dict(), os.path.join(os.getcwd(), 'decoder_%d.pt' % epoch))\n",
        "        torch.save(optimizer_decoder.state_dict(), os.path.join(os.getcwd(), 'optim_decoder_%d.pt' % epoch))\n",
        "\n",
        "        encoder.eval()\n",
        "        decoder.eval()\n",
        "\n",
        "        tol_label = np.array([], dtype=np.float)\n",
        "        tol_pred = np.array([], dtype=np.float)\n",
        "        tol_pred_prob = np.array([], dtype=np.float)\n",
        "\n",
        "        count = 0\n",
        "\n",
        "        for fft_data, labels_data in tqdm(dataloader_val):\n",
        "\n",
        "            fft_label = labels_data.numpy().astype(np.float)\n",
        "            labels_data = labels_data.float()\n",
        "\n",
        "            rgb = transform_norm(fft_data[:,:,:,0:256])\n",
        "            mask = fft_data[:,0,:,256:512]\n",
        "            mask[mask >= 0.5] = 1.0\n",
        "            mask[mask < 0.5] = 0.0\n",
        "            mask = mask.long()\n",
        "\n",
        "            if gpu_id >= 0:\n",
        "                rgb = rgb.cuda(gpu_id)\n",
        "                mask = mask.cuda(gpu_id)\n",
        "                labels_data = labels_data.cuda(gpu_id)\n",
        "\n",
        "            latent = encoder(rgb).reshape(-1, 2, 64, 16, 16)\n",
        "\n",
        "            zero_abs = torch.abs(latent[:,0]).view(latent.shape[0], -1)\n",
        "            zero = zero_abs.mean(dim=1)\n",
        "\n",
        "            one_abs = torch.abs(latent[:,1]).view(latent.shape[0], -1)\n",
        "            one = one_abs.mean(dim=1)\n",
        "\n",
        "            loss_act = act_loss_fn(zero, one, labels_data)\n",
        "            loss_act_data = loss_act.item()\n",
        "\n",
        "            y = torch.eye(2)\n",
        "            if gpu_id >= 0:\n",
        "                y = y.cuda(gpu_id)\n",
        "\n",
        "            y = y.index_select(dim=0, index=labels_data.data.long())\n",
        "\n",
        "            latent = (latent * y[:,:,None, None, None]).reshape(-1, 128, 16, 16)\n",
        "\n",
        "            seg, rect = decoder(latent)\n",
        "\n",
        "            loss_seg = seg_loss_fn(seg, mask)\n",
        "            loss_seg = loss_seg * gamma\n",
        "            loss_seg_data = loss_seg.item()\n",
        "\n",
        "            loss_rect = rect_loss_fn(rect, rgb)\n",
        "            loss_rect = loss_rect * gamma\n",
        "            loss_rect_data = loss_rect.item()\n",
        "\n",
        "            output_pred = np.zeros((fft_data.shape[0]), dtype=np.float)\n",
        "\n",
        "            for i in range(fft_data.shape[0]):\n",
        "                if one[i] >= zero[i]:\n",
        "                    output_pred[i] = 1.0\n",
        "                else:\n",
        "                    output_pred[i] = 0.0\n",
        "\n",
        "            tol_label = np.concatenate((tol_label, fft_label))\n",
        "            tol_pred = np.concatenate((tol_pred, output_pred))\n",
        "            \n",
        "            pred_prob = torch.softmax(torch.cat((zero.reshape(zero.shape[0],1), one.reshape(one.shape[0],1)), dim=1), dim=1)\n",
        "            tol_pred_prob = np.concatenate((tol_pred_prob, pred_prob[:,1].data.cpu().numpy()))\n",
        "\n",
        "            loss_act_test += loss_act_data\n",
        "            loss_seg_test += loss_seg_data\n",
        "            loss_rect_test += loss_rect_data\n",
        "            count += 1\n",
        "\n",
        "        acc_test = metrics.accuracy_score(tol_label, tol_pred)\n",
        "        loss_act_test /= count\n",
        "        loss_seg_test /= count\n",
        "        loss_rect_test /= count\n",
        "\n",
        "        fpr, tpr, thresholds = roc_curve(tol_label, tol_pred_prob, pos_label=1)\n",
        "        eer = brentq(lambda x : 1. - x - interp1d(fpr, tpr)(x), 0., 1.)\n",
        "\n",
        "        print('[Epoch %d] Train: act_loss: %.4f  seg_loss: %.4f  rect_loss: %.4f  acc: %.2f | Test: act_loss: %.4f  seg_loss: %.4f  rect_loss: %.4f  acc: %.2f  eer: %.2f'\n",
        "        % (epoch, loss_act_train, loss_seg_train, loss_rect_train, acc_train*100, loss_act_test, loss_seg_test, loss_rect_test, acc_test*100, eer*100))\n",
        "\n",
        "        text_writer.write('%d,%.4f,%.4f,%.4f,%.2f,%.4f,%.4f,%.4f,%.2f,%.2f\\n'\n",
        "        % (epoch, loss_act_train, loss_seg_train, loss_rect_train, acc_train*100, loss_act_test, loss_seg_test, loss_rect_test, acc_test*100, eer*100))\n",
        "\n",
        "        text_writer_train.flush()\n",
        "\n",
        "        ########################################################################\n",
        "\n",
        "        real_img = transform_tns(Image.open(os.path.join('test_img', 'real.jpg'))).unsqueeze(0)[:,:,:,0:256]\n",
        "        real_mask = transform_tns(Image.open(os.path.join('test_img', 'real.jpg'))).unsqueeze(0)[:,:,:,256:512]\n",
        "        fake_img = transform_tns(Image.open(os.path.join('test_img', 'fake.jpg'))).unsqueeze(0)[:,:,:,0:256]\n",
        "        fake_mask = transform_tns(Image.open(os.path.join('test_img', 'fake.jpg'))).unsqueeze(0)[:,:,:,256:512]\n",
        "\n",
        "        rgb = torch.cat((real_img, fake_img), dim=0)\n",
        "        rgb = transform_norm(rgb)\n",
        "\n",
        "        real_mask[real_mask >= 0.5] = 1.0\n",
        "        real_mask[real_mask < 0.5] = 0.0\n",
        "        real_mask = real_mask.long()\n",
        "\n",
        "        fake_mask[fake_mask >= 0.5] = 1.0\n",
        "        fake_mask[fake_mask < 0.5] = 0.0\n",
        "        fake_mask = fake_mask.long()\n",
        "\n",
        "        # real = 1, fake = 0\n",
        "        labels_data = torch.FloatTensor([1,0])\n",
        "\n",
        "        if gpu_id >= 0:\n",
        "          rgb = rgb.cuda(gpu_id)\n",
        "          labels_data = labels_data.cuda(gpu_id)\n",
        "\n",
        "        latent = encoder(rgb).reshape(-1, 2, 64, 16, 16)\n",
        "\n",
        "        zero_abs = torch.abs(latent[:,0]).view(latent.shape[0], -1)\n",
        "        zero = zero_abs.mean(dim=1)\n",
        "\n",
        "        one_abs = torch.abs(latent[:,1]).view(latent.shape[0], -1)\n",
        "        one = one_abs.mean(dim=1)\n",
        "\n",
        "        y = torch.eye(2)\n",
        "\n",
        "        if gpu_id >= 0:\n",
        "          y = y.cuda(gpu_id)\n",
        "\n",
        "        y = y.index_select(dim=0, index=labels_data.data.long())\n",
        "\n",
        "        latent = (latent * y[:,:,None, None, None]).reshape(-1, 128, 16, 16)\n",
        "\n",
        "        seg, rect = decoder(latent)\n",
        "\n",
        "        seg = seg[:,1,:,:].detach().cpu()\n",
        "        seg[seg >= 0.5] = 1.0\n",
        "        seg[seg < 0.5] = 0.0\n",
        "\n",
        "        rect = transform_unnorm(rect).detach().cpu()\n",
        "\n",
        "        real_seg = transform_pil(seg[0])\n",
        "        fake_seg = transform_pil(seg[1])\n",
        "\n",
        "        real_img = transform_pil(rect[0])\n",
        "        fake_img = transform_pil(rect[1])\n",
        "\n",
        "        real_seg.save(os.path.join(os.getcwd(), 'image', 'seg_real_' + str(epoch).zfill(3) + '.jpg'))\n",
        "        fake_seg.save(os.path.join(os.getcwd(), 'image', 'seg_fake_' + str(epoch).zfill(3) + '.jpg'))\n",
        "\n",
        "        real_img.save(os.path.join(os.getcwd(), 'image', 'real_' + str(epoch).zfill(3) + '.jpg'))\n",
        "        fake_img.save(os.path.join(os.getcwd(), 'image', 'fake_' + str(epoch).zfill(3) + '.jpg'))\n",
        "\n",
        "        encoder.train(mode=True)\n",
        "        decoder.train(mode=True)\n",
        "\n",
        "text_writer_train.close()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-25b7a011009d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mtol_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mfft_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_data\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0moptimizer_encoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'dataloader_train' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0Yh5s1xpJlE",
        "colab_type": "text"
      },
      "source": [
        "## 8. Start evaluation phase"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z24BbUe6pL0r",
        "colab_type": "code",
        "outputId": "ad5bba66-3567-4fe2-9931-9a6d820109f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254
        }
      },
      "source": [
        "encoder.load_state_dict(torch.load(os.path.join(os.getcwd(),'encoder_' + str(46) + '.pt')))\n",
        "encoder.eval()\n",
        "\n",
        "loss_act_test = 0.0\n",
        "\n",
        "tol_label = np.array([], dtype=np.float)\n",
        "tol_pred = np.array([], dtype=np.float)\n",
        "tol_pred_prob = np.array([], dtype=np.float)\n",
        "\n",
        "count = 0\n",
        "\n",
        "for fft_data, labels_data in tqdm(dataloader_test):\n",
        "\n",
        "        fft_label = labels_data.numpy().astype(np.float)\n",
        "        labels_data = labels_data.float()\n",
        "\n",
        "        rgb = transform_norm(fft_data[:,:,:,0:256])\n",
        "\n",
        "        if gpu_id >= 0:\n",
        "          rgb = rgb.cuda(gpu_id)\n",
        "          labels_data = labels_data.cuda(gpu_id)\n",
        "\n",
        "        latent = encoder(rgb).reshape(-1, 2, 64, 16, 16)\n",
        "\n",
        "        zero_abs = torch.abs(latent[:,0]).view(latent.shape[0], -1)\n",
        "        zero = zero_abs.mean(dim=1)\n",
        "\n",
        "        one_abs = torch.abs(latent[:,1]).view(latent.shape[0], -1)\n",
        "        one = one_abs.mean(dim=1)\n",
        "\n",
        "        loss_act = act_loss_fn(zero, one, labels_data)\n",
        "        loss_act_data = loss_act.item()\n",
        "\n",
        "        output_pred = np.zeros((fft_data.shape[0]), dtype=np.float)\n",
        "\n",
        "        for i in range(fft_data.shape[0]):\n",
        "            if one[i] >= zero[i]:\n",
        "                output_pred[i] = 1.0\n",
        "            else:\n",
        "                output_pred[i] = 0.0\n",
        "\n",
        "        tol_label = np.concatenate((tol_label, fft_label))\n",
        "        tol_pred = np.concatenate((tol_pred, output_pred))\n",
        "        \n",
        "        pred_prob = torch.softmax(torch.cat((zero.reshape(zero.shape[0],1), one.reshape(one.shape[0],1)), dim=1), dim=1)\n",
        "        tol_pred_prob = np.concatenate((tol_pred_prob, pred_prob[:,1].data.cpu().numpy()))\n",
        "\n",
        "        loss_act_test += loss_act_data\n",
        "        count += 1\n",
        "\n",
        "acc_test = metrics.accuracy_score(tol_label, tol_pred)\n",
        "loss_act_test /= count\n",
        "\n",
        "fpr, tpr, thresholds = roc_curve(tol_label, tol_pred_prob, pos_label=1)\n",
        "eer = brentq(lambda x : 1. - x - interp1d(fpr, tpr)(x), 0., 1.)\n",
        "\n",
        "print('[Epoch %d] act_loss: %.4f  acc: %.2f   eer: %.2f' % (opt.id, loss_act_test, acc_test*100, eer*100))\n",
        "text_writer_test.write('%d,%.4f,%.2f,%.2f\\n'% (46, loss_act_test, acc_test*100, eer*100))\n",
        "\n",
        "text_writer_test.flush()\n",
        "text_writer_test.close()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-b392017033a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'encoder_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m46\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mloss_act_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'encoder' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kS27O-NIX7kZ",
        "colab_type": "text"
      },
      "source": [
        "# Train DSP-FWA model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jm__-WYL--cr",
        "colab_type": "text"
      },
      "source": [
        "**Public repository:** \n",
        "https://github.com/danmohaha/DSP-FWA \n",
        "\n",
        "**Citation:**\n",
        "@inproceedings{li2019exposing,\n",
        "  title={Exposing DeepFake Videos By Detecting Face Warping Artifacts},\n",
        "  author={Li, Yuezun and Lyu, Siwei},\n",
        "  booktitle={IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)},\n",
        "  year={2019}\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9D96Qra_eQX",
        "colab_type": "text"
      },
      "source": [
        "## 1. Import dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFVOvZaSZADN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torchvision import models\n",
        "import torch.nn.functional as F\n",
        "import cv2, os, math, dlib\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebaRRrpS_iTR",
        "colab_type": "text"
      },
      "source": [
        "## 2. DSP-FWA Network architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zJ8n4EzZGjP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ResNet(nn.Module):\n",
        "    def __init__(self, layers=18, num_class=2, pretrained=True):\n",
        "        super(ResNet, self).__init__()\n",
        "        if layers == 18:\n",
        "            self.resnet = models.resnet18(pretrained=pretrained)\n",
        "        elif layers == 34:\n",
        "            self.resnet = models.resnet34(pretrained=pretrained)\n",
        "        elif layers == 50:\n",
        "            self.resnet = models.resnet50(pretrained=pretrained)\n",
        "        elif layers == 101:\n",
        "            self.resnet = models.resnet101(pretrained=pretrained)\n",
        "        elif layers == 152:\n",
        "            self.resnet = models.resnet152(pretrained=pretrained)\n",
        "        else:\n",
        "            raise ValueError('layers should be 18, 34, 50, 101.')\n",
        "        self.num_class = num_class\n",
        "        if layers in [18, 34]:\n",
        "            self.fc = nn.Linear(512, num_class)\n",
        "        if layers in [50, 101, 152]:\n",
        "            self.fc = nn.Linear(512 * 4, num_class)\n",
        "\n",
        "    def conv_base(self, x):\n",
        "        x = self.resnet.conv1(x)\n",
        "        x = self.resnet.bn1(x)\n",
        "        x = self.resnet.relu(x)\n",
        "        x = self.resnet.maxpool(x)\n",
        "\n",
        "        layer1 = self.resnet.layer1(x)\n",
        "        layer2 = self.resnet.layer2(layer1)\n",
        "        layer3 = self.resnet.layer3(layer2)\n",
        "        layer4 = self.resnet.layer4(layer3)\n",
        "        return layer1, layer2, layer3, layer4\n",
        "\n",
        "    def forward(self, x):\n",
        "        layer1, layer2, layer3, layer4 = self.conv_base(x)\n",
        "        x = self.resnet.avgpool(layer4)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "class SPPNet(nn.Module):\n",
        "    def __init__(self, backbone=101, num_class=2, pool_size=(1, 2, 6), pretrained=True):\n",
        "        # Only resnet is supported in this version\n",
        "        super(SPPNet, self).__init__()\n",
        "        if backbone in [18, 34, 50, 101, 152]:\n",
        "            self.resnet = ResNet(backbone, num_class, pretrained)\n",
        "        else:\n",
        "            raise ValueError('Resnet{} is not supported yet.'.format(backbone))\n",
        "\n",
        "        if backbone in [18, 34]:\n",
        "            self.c = 512\n",
        "        if backbone in [50, 101, 152]:\n",
        "            self.c = 2048\n",
        "\n",
        "        self.spp = SpatialPyramidPool2D(out_side=pool_size)\n",
        "        num_features = self.c * (pool_size[0] ** 2 + pool_size[1] ** 2 + pool_size[2] ** 2)\n",
        "        self.classifier = nn.Linear(num_features, num_class)\n",
        "\n",
        "    def forward(self, x):\n",
        "        _, _, _, x = self.resnet.conv_base(x)\n",
        "        x = self.spp(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class SpatialPyramidPool2D(nn.Module):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        out_side (tuple): Length of side in the pooling results of each pyramid layer.\n",
        "\n",
        "    Inputs:\n",
        "        - `input`: the input Tensor to invert ([batch, channel, width, height])\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, out_side):\n",
        "        super(SpatialPyramidPool2D, self).__init__()\n",
        "        self.out_side = out_side\n",
        "\n",
        "    def forward(self, x):\n",
        "        # batch_size, c, h, w = x.size()\n",
        "        out = None\n",
        "        for n in self.out_side:\n",
        "            w_r, h_r = map(lambda s: math.ceil(s / n), x.size()[2:])  # Receptive Field Size\n",
        "            s_w, s_h = map(lambda s: math.floor(s / n), x.size()[2:])  # Stride\n",
        "            max_pool = nn.MaxPool2d(kernel_size=(w_r, h_r), stride=(s_w, s_h))\n",
        "            y = max_pool(x)\n",
        "            if out is None:\n",
        "                out = y.view(y.size()[0], -1)\n",
        "            else:\n",
        "                out = torch.cat((out, y.view(y.size()[0], -1)), 1)\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8cXeGs1_spl",
        "colab_type": "text"
      },
      "source": [
        "## 3. Default settings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5OjufhnE_yn4",
        "colab_type": "code",
        "outputId": "f95117b1-041d-4be5-a9b4-c67359976f39",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "ckpt_name = 'SPP-res50.pth'\n",
        "model_path = os.path.join(os.getcwd(), ckpt_name)\n",
        "f_path = os.getcwd() + '/celeb-df/validation/1_fake_videos/id0_id16_0005.mp4'\n",
        "print('Testing: ' + f_path)\n",
        "suffix = f_path.split('.')[-1]\n",
        "num_class = 2\n",
        "layers = 50"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing: /content/celeb-df/validation/1_fake_videos/id0_id16_0005.mp4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVw0i5nHBDs9",
        "colab_type": "text"
      },
      "source": [
        "## Start pre-processing phase"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8-IwGH6BFlK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class pv():\n",
        "    def crop_video(pathIn, pathOut, pos, size):\n",
        "        \"\"\"\n",
        "        Crop video\n",
        "        :param pathIn:\n",
        "        :param pathOut:\n",
        "        :param pos: (left, top, right, bottom)\n",
        "        :return:\n",
        "        \"\"\"\n",
        "\n",
        "        imgs, frame_num, fps, width, height = parse_vid(pathIn)\n",
        "\n",
        "        for i, image in enumerate(imgs):\n",
        "            y1 = np.int32(pos[0])\n",
        "            x1 = np.int32(pos[1])\n",
        "            y2 = np.int32(pos[2])\n",
        "            x2 = np.int32(pos[3])\n",
        "            roi = image[y1:y2, x1:x2, :]\n",
        "            if size is not 'None':\n",
        "                roi = cv2.resize(roi, (size[1], size[0]))\n",
        "            imgs[i] = roi\n",
        "\n",
        "        gen_vid(pathOut, imgs, fps, width, height)\n",
        "\n",
        "\n",
        "    def get_video_dims(video_path):\n",
        "        vidcap = cv2.VideoCapture(video_path)\n",
        "        width = np.int32(vidcap.get(cv2.CAP_PROP_FRAME_WIDTH))  # float\n",
        "        height = np.int32(vidcap.get(cv2.CAP_PROP_FRAME_HEIGHT))  # float\n",
        "        vidcap.release()\n",
        "        return width, height\n",
        "\n",
        "\n",
        "    def get_video_frame_nums(video_path):\n",
        "        vidcap = cv2.VideoCapture(video_path)\n",
        "        frame_num = int(vidcap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "        vidcap.release()\n",
        "        return frame_num\n",
        "\n",
        "\n",
        "    def get_video_fps(video_path):\n",
        "        vidcap = cv2.VideoCapture(video_path)\n",
        "        fps = vidcap.get(cv2.CAP_PROP_FPS)\n",
        "        vidcap.release()\n",
        "        return fps\n",
        "\n",
        "\n",
        "    def parse_vid(video_path):\n",
        "        vidcap = cv2.VideoCapture(video_path)\n",
        "        frame_num = int(vidcap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "        fps = vidcap.get(cv2.CAP_PROP_FPS)\n",
        "        width = np.int32(vidcap.get(cv2.CAP_PROP_FRAME_WIDTH)) # float\n",
        "        height = np.int32(vidcap.get(cv2.CAP_PROP_FRAME_HEIGHT))  # float\n",
        "        imgs = []\n",
        "        while True:\n",
        "            success, image = vidcap.read()\n",
        "            if success:\n",
        "                imgs.append(image)\n",
        "            else:\n",
        "                break\n",
        "\n",
        "        vidcap.release()\n",
        "        if len(imgs) != frame_num:\n",
        "            frame_num = len(imgs)\n",
        "        return imgs, frame_num, fps, width, height\n",
        "\n",
        "\n",
        "    def parse_vid_into_imgs(video_path, folder, im_name='{:05d}.jpg'):\n",
        "        imgs, frame_num, fps, width, height = parse_vid(video_path)\n",
        "        for id, im in enumerate(imgs):\n",
        "            im_name = im_name.format(id)\n",
        "            cv2.imwrite(folder + '/' + im_name, im)\n",
        "        print('Save original images to folder {}'.format(folder))\n",
        "\n",
        "\n",
        "    def gen_vid(video_path, imgs, fps, width=None, height=None):\n",
        "        # Combine video\n",
        "        ext = Path(video_path).suffix\n",
        "        if ext == '.mp4':\n",
        "            fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Be sure to use lower case\n",
        "        elif ext == '.avi':\n",
        "            fourcc = cv2.VideoWriter_fourcc(*'MJPG')  #*'XVID')\n",
        "        else:\n",
        "            # if not .mp4 or avi, we force it to mp4\n",
        "            video_path = video_path.replace(ext, '.mp4')\n",
        "            fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Be sure to use lower case\n",
        "        if width is None or height is None:\n",
        "            height, width= imgs[0].shape[:2]\n",
        "        else:\n",
        "            imgs_ = [cv2.resize(img, (width, height)) for img in imgs]\n",
        "            imgs = imgs_\n",
        "\n",
        "        out = cv2.VideoWriter(video_path, fourcc, fps, (np.int32(width), np.int32(height)))\n",
        "\n",
        "        for image in imgs:\n",
        "            out.write(np.uint8(image))  # Write out frame to video\n",
        "\n",
        "        # Release everything if job is finished\n",
        "        out.release()\n",
        "        print('The output video is ' + video_path)\n",
        "\n",
        "\n",
        "    def gen_vid_from_folder(video_path, img_dir, ext, fps, width=None, height=None):\n",
        "        imgs_path = sorted(Path(img_dir).glob('*' + ext))\n",
        "        imgs = [cv2.imread(str(p)) for p in imgs_path]\n",
        "        gen_vid(video_path, imgs, fps, width, height)\n",
        "\n",
        "\n",
        "    def resize_video(video_path, w=None, h=None, scale=1., out_path=None):\n",
        "        imgs, frame_num, fps, width, height = parse_vid(video_path)\n",
        "        # Resize imgs\n",
        "        if w is None or h is None:\n",
        "            width, height = int(width * scale), int(height * scale)\n",
        "            for i, im in enumerate(imgs):\n",
        "                im = cv2.resize(im, None, None, fx=scale, fy=scale)\n",
        "                imgs[i] = im\n",
        "        else:\n",
        "            width, height = w, h\n",
        "            for i, im in enumerate(imgs):\n",
        "                im = cv2.resize(im, (w, h))\n",
        "                imgs[i] = im\n",
        "        if out_path:\n",
        "            gen_vid(out_path, imgs, fps, width, height)\n",
        "        return imgs, frame_num, fps, width, height\n",
        "\n",
        "\n",
        "    def extract_key_frames(video_path, len_window=50):\n",
        "        \"\"\"\n",
        "        The frames which the average interframe difference are local maximum are\n",
        "        considered to be key frames.\n",
        "        It should be noted that smoothing the average difference value before\n",
        "        calculating the local maximum can effectively remove noise to avoid\n",
        "        repeated extraction of frames of similar scenes.\n",
        "        \"\"\"\n",
        "        imgs, frame_num, fps, width, height = parse_vid(video_path)\n",
        "        frame_diffs = []\n",
        "        for i in range(1, len(imgs)):\n",
        "            curr_frame = cv2.cvtColor(imgs[i], cv2.COLOR_BGR2LUV)\n",
        "            prev_frame = cv2.cvtColor(imgs[i - 1], cv2.COLOR_BGR2LUV)\n",
        "            # logic here\n",
        "            diff = cv2.absdiff(curr_frame, prev_frame)\n",
        "            diff_sum = np.sum(diff)\n",
        "            diff_sum_mean = diff_sum / (diff.shape[0] * diff.shape[1])\n",
        "            frame_diffs.append(diff_sum_mean)\n",
        "        from scipy.signal import argrelextrema\n",
        "        # compute keyframe\n",
        "        diff_array = np.array(frame_diffs)\n",
        "\n",
        "        def smooth(x, window_len=13, window='hanning'):\n",
        "            s = np.r_[2 * x[0] - x[window_len:1:-1],\n",
        "                      x, 2 * x[-1] - x[-1:-window_len:-1]]\n",
        "            if window == 'flat':  # moving average\n",
        "                w = np.ones(window_len, 'd')\n",
        "            else:\n",
        "                w = getattr(np, window)(window_len)\n",
        "            y = np.convolve(w / w.sum(), s, mode='same')\n",
        "            return y[window_len - 1:-window_len + 1]\n",
        "\n",
        "        sm_diff_array = smooth(diff_array, len_window)\n",
        "        frame_indexes = np.asarray(argrelextrema(sm_diff_array, np.greater))[0]\n",
        "        key_frames = []\n",
        "        for i in frame_indexes:\n",
        "            key_frames.append(imgs[i])\n",
        "        return key_frames"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCs3DfQxCt7w",
        "colab_type": "text"
      },
      "source": [
        "## Start training phase"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmuaPHGwCv0D",
        "colab_type": "code",
        "outputId": "144f6d39-1f1e-4a78-e09b-ff1329ebf1c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "net = SPPNet(backbone=layers, num_class=num_class)\n",
        "net = net.cuda()\n",
        "net.eval()\n",
        "\n",
        "if os.path.isfile(model_path):\n",
        "  print(\"=> loading checkpoint '{}'\".format(model_path))\n",
        "  checkpoint = torch.load(model_path)\n",
        "  start_epoch = checkpoint['epoch']\n",
        "  net.load_state_dict(checkpoint['net'])\n",
        "  print(\"=> loaded checkpoint '{}' (epoch {})\".format(model_path, start_epoch))\n",
        "else:\n",
        "  raise ValueError(\"=> no checkpoint found at '{}'\".format(model_path))\n",
        "\n",
        "def im_test(net, im, args):\n",
        "    face_info = lib.align(im[:, :, (2,1,0)], front_face_detector, lmark_predictor)\n",
        "    # Samples\n",
        "    if len(face_info) != 1:\n",
        "        prob = -1\n",
        "    else:\n",
        "        _, point = face_info[0]\n",
        "        rois = []\n",
        "        for i in range(sample_num):\n",
        "            roi, _ = lib.cut_head([im], point, i)\n",
        "            rois.append(cv2.resize(roi[0], (args.input_size, args.input_size)))\n",
        "\n",
        "        # vis_ = np.concatenate(rois, 1)\n",
        "        # cv2.imwrite('vis.jpg', vis_)\n",
        "\n",
        "        bgr_mean = np.array([103.939, 116.779, 123.68])\n",
        "        bgr_mean = bgr_mean[np.newaxis, :, np.newaxis, np.newaxis]\n",
        "        bgr_mean = torch.from_numpy(bgr_mean).float().cuda()\n",
        "\n",
        "        rois = torch.from_numpy(np.array(rois)).float().cuda()\n",
        "        rois = rois.permute((0, 3, 1, 2))\n",
        "        prob = net(rois - bgr_mean)\n",
        "        prob = F.softmax(prob, dim=1)\n",
        "        prob = prob.data.cpu().numpy()\n",
        "        prob = 1 - np.mean(np.sort(prob[:, 0])[np.round(sample_num / 2).astype(int):])\n",
        "    return prob, face_info\n",
        "\n",
        "# Parse video\n",
        "imgs, frame_num, fps, width, height = pv.parse_vid(f_path)\n",
        "probs = []\n",
        "for fid, im in enumerate(imgs):\n",
        "  print('Frame: ' + str(fid))\n",
        "  prob, face_info = im_test(net, im, args)\n",
        "  probs.append(prob)\n",
        "print(probs)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "=> loading checkpoint '/content/SPP-res50.pth'\n",
            "=> loaded checkpoint '/content/SPP-res50.pth' (epoch 49)\n",
            "Frame: 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-2922c0af1c08>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mim\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Frame: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m   \u001b[0mprob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mface_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mim_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m   \u001b[0mprobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'args' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MtOdbT3qCwS9",
        "colab_type": "text"
      },
      "source": [
        "## Start evaluation phase"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gf_OkND4Cyc_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KaEVoG_NYAFW",
        "colab_type": "text"
      },
      "source": [
        "# Train Ictu Oculi model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ll4p6sX5_QLO",
        "colab_type": "text"
      },
      "source": [
        "**Public repository:** \n",
        "https://github.com/danmohaha/WIFS2018_In_Ictu_Oculi \n",
        "\n",
        "**Citation:**\n",
        "@inproceedings{li2018ictu,\n",
        "  title={In Ictu Oculi: Exposing AI Generated Fake Face Videos by Detecting Eye Blinking},\n",
        "  author={Li, Yuezun and Chang, Ming-Ching and Lyu, Siwei},\n",
        "  Booktitle={IEEE International Workshop on Information Forensics and Security (WIFS)},\n",
        "  year={2018}\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eo_aDQ--IxLM",
        "colab_type": "text"
      },
      "source": [
        "## 1. Import dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TH4xz_HKIzId",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ln8fg8N8LD9M",
        "colab_type": "text"
      },
      "source": [
        "## 2. Ictu Oculi Network architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZ3rQAePLGvZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# =============================\n",
        "# VGG16 network structure\n",
        "# =============================\n",
        "\n",
        "class vgg16():\n",
        "    def get_vgg16_conv5(input, params):\n",
        "        layers = edict()\n",
        "\n",
        "        layers.conv1_1 = ops.conv2D(input=input, shape=(3, 3, 64), name='conv1_1', params=params)\n",
        "        layers.conv1_1_relu = ops.activate(input=layers.conv1_1, name='conv1_1_relu', act_type='relu')\n",
        "        layers.conv1_2 = ops.conv2D(input=layers.conv1_1_relu, shape=(3, 3, 64), name='conv1_2', params=params)\n",
        "        layers.conv1_2_relu = ops.activate(input=layers.conv1_2, name='conv1_2_relu', act_type='relu')\n",
        "        layers.pool1 = ops.max_pool(input=layers.conv1_2_relu, name='pool1')\n",
        "\n",
        "        layers.conv2_1 = ops.conv2D(input=layers.pool1, shape=(3, 3, 128), name='conv2_1', params=params)\n",
        "        layers.conv2_1_relu = ops.activate(input=layers.conv2_1, name='conv2_1_relu', act_type='relu')\n",
        "        layers.conv2_2 = ops.conv2D(input=layers.conv2_1_relu, shape=(3, 3, 128), name='conv2_2', params=params)\n",
        "        layers.conv2_2_relu = ops.activate(input=layers.conv2_2, name='conv2_2_relu', act_type='relu')\n",
        "        layers.pool2 = ops.max_pool(input=layers.conv2_2_relu, name='pool2')\n",
        "\n",
        "        layers.conv3_1 = ops.conv2D(input=layers.pool2, shape=(3, 3, 256), name='conv3_1', params=params)\n",
        "        layers.conv3_1_relu = ops.activate(input=layers.conv3_1, name='conv3_1_relu', act_type='relu')\n",
        "        layers.conv3_2 = ops.conv2D(input=layers.conv3_1_relu, shape=(3, 3, 256), name='conv3_2', params=params)\n",
        "        layers.conv3_2_relu = ops.activate(input=layers.conv3_2, name='conv3_2_relu', act_type='relu')\n",
        "        layers.conv3_3 = ops.conv2D(input=layers.conv3_2_relu, shape=(3, 3, 256), name='conv3_3', params=params)\n",
        "        layers.conv3_3_relu = ops.activate(input=layers.conv3_3, name='conv3_3_relu', act_type='relu')\n",
        "        layers.pool3 = ops.max_pool(input=layers.conv3_3_relu, name='pool3')\n",
        "\n",
        "        layers.conv4_1 = ops.conv2D(input=layers.pool3, shape=(3, 3, 512), name='conv4_1', params=params)\n",
        "        layers.conv4_1_relu = ops.activate(input=layers.conv4_1, name='conv4_1_relu', act_type='relu')\n",
        "        layers.conv4_2 = ops.conv2D(input=layers.conv4_1_relu, shape=(3, 3, 512), name='conv4_2', params=params)\n",
        "        layers.conv4_2_relu = ops.activate(input=layers.conv4_2, name='conv4_2_relu', act_type='relu')\n",
        "        layers.conv4_3 = ops.conv2D(input=layers.conv4_2_relu, shape=(3, 3, 512), name='conv4_3', params=params)\n",
        "        layers.conv4_3_relu = ops.activate(input=layers.conv4_3, name='conv4_3_relu', act_type='relu')\n",
        "        layers.pool4 = ops.max_pool(input=layers.conv4_3_relu, name='pool4')\n",
        "\n",
        "        layers.conv5_1 = ops.conv2D(input=layers.pool4, shape=(3, 3, 512), name='conv5_1', params=params)\n",
        "        layers.conv5_1_relu = ops.activate(input=layers.conv5_1, name='conv5_1_relu', act_type='relu')\n",
        "        layers.conv5_2 = ops.conv2D(input=layers.conv5_1_relu, shape=(3, 3, 512), name='conv5_2', params=params)\n",
        "        layers.conv5_2_relu = ops.activate(input=layers.conv5_2, name='conv5_2_relu', act_type='relu')\n",
        "        layers.conv5_3 = ops.conv2D(input=layers.conv5_2_relu, shape=(3, 3, 512), name='conv5_3', params=params)\n",
        "        layers.conv5_3_relu = ops.activate(input=layers.conv5_3, name='conv5_3_relu', act_type='relu')\n",
        "\n",
        "        return layers\n",
        "\n",
        "    def get_vgg16_pool5(input, params):\n",
        "        layers = get_vgg16_conv5(input, params)\n",
        "        layers.pool5 = ops.max_pool(input=layers.conv5_3_relu, name='pool5')\n",
        "\n",
        "        return layers\n",
        "\n",
        "    def get_prob(input, params, num_class=1000, is_train=True):\n",
        "        # Get pool5\n",
        "        layers = get_vgg16_pool5(input, params)\n",
        "        layers.fc6 = ops.fully_connected(input=layers.pool5, num_neuron=4096, name='fc6', params=params)\n",
        "        if is_train:\n",
        "            layers.fc6 = tf.nn.dropout(layers.fc6, keep_prob=0.5)\n",
        "        layers.fc6_relu = ops.activate(input=layers.fc6, act_type='relu', name='fc6_relu')\n",
        "        layers.fc7 = ops.fully_connected(input=layers.fc6_relu, num_neuron=4096, name='fc7', params=params)\n",
        "        if is_train:\n",
        "            layers.fc7 = tf.nn.dropout(layers.fc7, keep_prob=0.5)\n",
        "        layers.fc7_relu = ops.activate(input=layers.fc7, act_type='relu', name='fc7_relu')\n",
        "        layers.fc8 = ops.fully_connected(input=layers.fc7_relu, num_neuron=num_class, name='fc8', params=params)\n",
        "        layers.prob = tf.nn.softmax(layers.fc8)\n",
        "        return layers\n",
        "\n",
        "class BlinkCNN(object):\n",
        "    \"\"\"\n",
        "    CNN for eye blinking detection\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 is_train\n",
        "                 ):\n",
        "\n",
        "        self.img_size = [224, 224, 3]\n",
        "        self.num_classes = 2\n",
        "        self.is_train = is_train\n",
        "\n",
        "        self.layers = {}\n",
        "        self.params = {}\n",
        "\n",
        "        base = vgg16()\n",
        "\n",
        "    def build(self):\n",
        "        # Input\n",
        "        self.input = tf.placeholder(dtype=tf.float32, shape=[None, self.img_size[0], self.img_size[1], self.img_size[2]])\n",
        "        self.layers = base.get_prob(self.input, self.params, self.num_classes, self.is_train)\n",
        "        self.prob = self.layers.prob\n",
        "        self.gt = tf.placeholder(dtype=tf.int32, shape=[None])\n",
        "        self.var_list = tf.trainable_variables()\n",
        "\n",
        "    def loss(self):\n",
        "        self.net_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.gt, logits=self.layers.fc8)\n",
        "        self.net_loss = tf.reduce_mean(self.net_loss)\n",
        "        tf.losses.add_loss(self.net_loss)\n",
        "        # L2 weight regularize\n",
        "        self.L2_loss = tf.reduce_mean([self.cfg.TRAIN.BETA * tf.nn.l2_loss(v)\n",
        "                     for v in tf.trainable_variables() if 'weights' in v.name])\n",
        "        tf.losses.add_loss(self.L2_loss)\n",
        "        self.total_loss = tf.losses.get_total_loss()\n",
        "\n",
        "class BlinkLRCN(object):\n",
        "    \"\"\"\n",
        "    LRCN for eye blinking detection\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 is_train\n",
        "                 ):\n",
        "\n",
        "        cfg_file = os.path.join(pwd, 'blink_lrcn.yml')\n",
        "        with open(cfg_file, 'r') as f:\n",
        "            cfg = edict(yaml.load(f))\n",
        "\n",
        "        self.cfg = cfg\n",
        "        self.img_size = cfg.IMG_SIZE\n",
        "        self.num_classes = cfg.NUM_CLASS\n",
        "        self.is_train = is_train\n",
        "\n",
        "        self.rnn_type = cfg.RNN_TYPE\n",
        "        self.max_time = cfg.MAX_TIME\n",
        "        self.hidden_unit = cfg.HIDDEN_UNIT\n",
        "\n",
        "        if self.is_train:\n",
        "            self.batch_size = cfg.TRAIN.BATCH_SIZE\n",
        "        else:\n",
        "            self.batch_size = cfg.TEST.BATCH_SIZE\n",
        "        self.layers = {}\n",
        "        self.params = {}\n",
        "\n",
        "    def build(self):\n",
        "        self.input = tf.placeholder(dtype=tf.float32,\n",
        "                                    shape=[self.batch_size, self.max_time, self.img_size[0], self.img_size[1], self.img_size[2]])\n",
        "        self.blined_gt = tf.placeholder(dtype=tf.int32, shape=[self.batch_size])\n",
        "        self.eye_state_gt = tf.placeholder(dtype=tf.int32, shape=[self.batch_size, self.max_time])\n",
        "        self.seq_len = tf.placeholder(dtype=tf.int32, shape=[self.batch_size])\n",
        "\n",
        "        self.vgg16_fc6 = self._vgg16(self.input)\n",
        "        self.rnn_out = self._rnn_cell(self.vgg16_fc6)\n",
        "        self.out = self._fc(self.rnn_out)\n",
        "        self.prob = tf.nn.softmax(self.out, dim=-1)\n",
        "\n",
        "    def _vgg16(self, input):\n",
        "        # Reshape from NxTxHxWxC to (NxT)xHxWxC\n",
        "        input = tf.reshape(input, [-1, self.img_size[0], self.img_size[1], self.img_size[2]])\n",
        "        layers = base.get_vgg16_pool5(input, self.params)\n",
        "        layers.fc6 = net_ops.fully_connected(input=layers.pool5, num_neuron=4096, name='fc6', params=self.params)\n",
        "        if self.is_train:\n",
        "            layers.fc6 = tf.nn.dropout(layers.fc6, keep_prob=0.5)\n",
        "        layers.fc6_relu = net_ops.activate(input=layers.fc6, act_type='relu', name='fc6_relu')\n",
        "        cnn_out = tf.reshape(layers.fc6_relu, [-1, self.max_time, 4096])\n",
        "        return cnn_out\n",
        "\n",
        "    def _rnn_cell(self, input):\n",
        "        with tf.variable_scope('rnn_cell'):\n",
        "            size = np.prod(input.get_shape().as_list()[2:])\n",
        "            rnn_inputs = tf.reshape(input, (-1, self.max_time, size))\n",
        "            if self.rnn_type == 'LSTM':\n",
        "                cell = tf.contrib.rnn.LSTMCell(self.hidden_unit)\n",
        "            elif self.rnn_type == 'GRU':\n",
        "                cell = tf.contrib.rnn.GRUCell(self.hidden_unit)\n",
        "            else:\n",
        "                raise ValueError('We only support LSTM or GRU...')\n",
        "            rnn_outputs, _ = tf.nn.dynamic_rnn(\n",
        "                cell,\n",
        "                rnn_inputs,\n",
        "                sequence_length=self.seq_len,\n",
        "                dtype = tf.float32\n",
        "            )\n",
        "            return rnn_outputs\n",
        "\n",
        "    def _avg_rnn_out(self, rnn_out):\n",
        "        seq_len = tf.cast(self.seq_len, dtype=tf.float32)\n",
        "        avg = tf.reduce_sum(rnn_out, axis=1) / tf.expand_dims(seq_len, axis=-1)\n",
        "        return avg\n",
        "\n",
        "    def _fc(self, input):\n",
        "        # Reshape from NxTx256 to (NxT)x256\n",
        "        input = tf.reshape(input, [-1, self.hidden_unit])\n",
        "        out = net_ops.fully_connected(input=input, num_neuron=self.num_classes, name='fc_after_rnn', params=self.params)\n",
        "        out = tf.reshape(out, [-1, self.max_time, self.num_classes])\n",
        "        return out\n",
        "\n",
        "    def loss(self):\n",
        "        self.net_loss = []\n",
        "        for batch_id in range(self.batch_size):\n",
        "            out_cur = self.out[batch_id, :, :]\n",
        "            eye_state_cur = self.eye_state_gt[batch_id, :]\n",
        "            weights = tf.gather(tf.constant(self.cfg.TRAIN.CLASS_WEIGHTS, dtype=tf.float32), eye_state_cur)\n",
        "            loss_per_batch = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=eye_state_cur, logits=out_cur)  # T x num_class\n",
        "            loss_per_batch = loss_per_batch * weights\n",
        "            # Select loss by real len\n",
        "            seq_len = tf.cast(self.seq_len[batch_id], dtype=tf.float32)\n",
        "            tf_idx = tf.range(0, self.seq_len[batch_id])\n",
        "            loss_per_batch = tf.reduce_sum(tf.gather(loss_per_batch, tf_idx, axis=0)) / seq_len\n",
        "            self.net_loss.append(loss_per_batch)\n",
        "        self.net_loss = tf.reduce_mean(self.net_loss)\n",
        "        tf.losses.add_loss(self.net_loss)\n",
        "        # L2 weight regularize\n",
        "        self.L2_loss = tf.reduce_mean([self.cfg.TRAIN.BETA * tf.nn.l2_loss(v)\n",
        "                                       for v in tf.trainable_variables() if 'weights' in v.name or 'kernel' in v.name])\n",
        "        tf.losses.add_loss(self.L2_loss)\n",
        "        self.total_loss = tf.losses.get_total_loss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZUzXHa2LHLz",
        "colab_type": "text"
      },
      "source": [
        "## 3. Default settings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eLN3_5hOLIpv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2JGepy_PdYU",
        "colab_type": "text"
      },
      "source": [
        "## Start training phase"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4nG-Bn9PgCY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with tf.Session() as sess:\n",
        "        # Build network\n",
        "        net = BlinkCNN(is_train=True)\n",
        "        net.build()\n",
        "\n",
        "        # Init solver\n",
        "        solver = Solver(sess=sess, net=net)\n",
        "        solver.init()\n",
        "\n",
        "        # Eye state data generator\n",
        "        data_gen = EyeData(\n",
        "            anno_path='sample_eye_data/train.p',\n",
        "            data_dir='sample_eye_data/',\n",
        "            batch_size=net.cfg.TRAIN.BATCH_SIZE,\n",
        "            is_augment=True,\n",
        "            is_shuffle=True\n",
        "        )\n",
        "\n",
        "        print('Training...')\n",
        "        # Training\n",
        "        batch_num = data_gen.batch_num\n",
        "        summary_idx = 0\n",
        "        for epoch in range(solver.cfg.TRAIN.NUM_EPOCH):\n",
        "            for i in range(batch_num):\n",
        "                im_list, label_list, im_name_list \\\n",
        "                    = data_gen.get_batch(i, size=net.cfg.IMG_SIZE[:2])\n",
        "                uvis.vis_im(im_list, 'tmp')\n",
        "                _, summary, prob, net_loss = solver.train(im_list, label_list)\n",
        "                solver.writer.add_summary(summary, summary_idx)\n",
        "                summary_idx += 1\n",
        "                pred_label = np.argmax(prob, axis=-1)\n",
        "                print('====================================')\n",
        "                print('Net loss: {}'.format(net_loss))\n",
        "                print('Real label: {}'.format(label_list))\n",
        "                print('Pred label: {}'.format(pred_label))\n",
        "                print('Epoch: {}'.format(epoch))\n",
        "            if epoch % solver.cfg.TRAIN.SAVE_INTERVAL == 0:\n",
        "                solver.save(epoch)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yuhfeqwT8Sv",
        "colab_type": "text"
      },
      "source": [
        "## Start evaluating phase"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzjRiLEsUBe9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}